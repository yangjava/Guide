# 分库分表

随着公司业务快速发展，数据库中的数据量猛增，访问性能也变慢了，优化迫在眉睫。原因在于关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到 **1000W** 或 **100G** 以后，由于查询维度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。解决上述问题通常有以下两种方案：

- **通过提升服务器硬件能力来提高数据处理能力，比如增加存储容量 、CPU 等，这种方案成本很高，并且如果瓶颈在 MySQL 本身那么提高硬件也是有很的**

- **把数据分散在不同的数据库中，使得单一数据库的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的**



**解决方案**

- **垂直分表：** 可以把一个宽表的字段按访问频次、是否是大字段的原则拆分为多个表，这样既能使业务清晰，还能提升部分性能。拆分后，尽量从业务角度避免联查，否则性能方面将得不偿失

- **垂直分库：** 可以把多个表按业务耦合松紧归类，分别存放在不同的库，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能，同时能提高整体架构的业务清晰度，不同的业务库可根据自身情况定制优化方案。但是它需要解决跨库带来的所有复杂问题

- **水平分库：** 可以把一个表的数据(按数据行)分到多个不同的库，每个库只有这个表的部分数据，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能。它不仅需要解决跨库带来的所有复杂问题，还要解决数据路由的问题

- **水平分表：** 可以把一个表的数据(按数据行)分到多个同一个数据库的多张表中，每个表只有这个表的部分数据，这样做能小幅提升性能，它仅仅作为水平分库的一个补充优化

一般来说，在系统设计阶段就应该根据业务耦合松紧来确定垂直分库，垂直分表方案，在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技术等方案。若数据量极大，且持续增长，再考虑水平分库水平分表方案。



**拆分目的**

- **垂直拆分：业务数据解耦**

- **水平拆分：解决容量和性能压力**



**分库分表前的问题**

- 用户请求量太大
  - 因为单服务器TPS，内存，IO都是有限的
  - **解决方法**：分散请求到多个服务器上； 其实用户请求和执行一个sql查询是本质是一样的，都是请求一个资源，只是用户请求还会经过网关，路由，http服务器等

- 单库太大
  - 单个数据库处理能力有限；单库所在服务器上磁盘空间不足；单库上操作的IO瓶颈
  - **解决方法**：切分成更多更小的库

- 单表太大
  - CRUD都成问题；索引膨胀，查询超时
  - **解决方法**：切分成多个数据集更小的表



## 垂直拆分

### 垂直分表

**垂直分表：将一个表按照字段分成多表，每个表存储其中一部分字段。**

我们拿网上商城举例：用户在浏览商品列表时，通常只会快速浏览商品名称、商品图片、商品价格等其他字段信息，这些字段数据访问频次较高。当只有对某商品感兴趣时才会查看该商品的详细描述。因此，商品信息中商品描述字段访问频次较低，且该字段存储占用空间较大，访问单个数据 IO 时间较长。

由于这两种数据的访问频次的不同，我们可以将商品信息表分成如下 2 张表：

![垂直分表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/垂直分表.jpg)

**优化提升：**

1. 为了避免 IO 争抢并减少锁表的几率，查看详情的用户与商品信息浏览互不影响
2. 充分发挥热门数据的操作效率，商品信息的操作的高效率不会被商品描述的低效率所拖累

**拆分原则：**

1. 把不常用的字段单独放在一张表
2. 把text，blob等大字段拆分出来放在附表中
3. 经常组合查询的列放在一张表中



### 垂直分库

**垂直分库：按照业务将表进行分类，分布到不同数据库上面，每个库可以放在不同服务器上，它的核心理念是专库专用。**

通过垂直分表性能得到了一定程度的提升，但是还没有达到要求，并且磁盘空间也快不够了，因为数据还是始终限制在一台服务器，库内垂直分表只解决了单一表数据量过大的问题，但没有将表分布到不同的服务器上，因此每个表还是竞争同一个物理机的CPU、内存、网络IO、磁盘。

继续拿商城举例：一个商城系统通常都包含用户信息表和商品信息表，这两张表在业务上是独立的，因此我们可以将它们拆开分到2个不同的库中。

![垂直分库](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/垂直分库.jpg)

**优化提升：**

1. 解决业务层面的耦合，业务清晰
2. 能对不同业务的数据进行分级管理、维护、监控、扩展等
3. 高并发场景下，垂直分库一定程度的提升IO、数据库连接数、降低单机硬件资源的瓶颈



## 水平拆分

### 水平分库

**水平分库：把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。**

经过垂直分库后，数据库性能问题得到一定程度的解决，但是随着业务量的增长，单库存储数据已经超出预估。单台服务器已经无法支撑。此时该如何优化？垂直拆分已达到极限，只能从水平维度拆分。

继续拿商城举例：我们要查询某个商品信息时，需要分析这条商品信息的ID。如果ID为双数，将此操作映射至DB_1(商品库1)。如果店铺ID为单数，将操作映射至DB_2(商品库2)。此操作要访问数据库名称的表达式为DB_[商品信息ID % 2 + 1]。

![水平分库](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/水平分库.jpg)

**优化提升：**

1. 解决了单库大数据，高并发的性能瓶颈
2. 提高了系统的稳定性及可用性



### 水平分表

**水平分表：在同一个数据库内，把同一个表的数据按一定规则拆到多个表中。**

即便水平分库，随着业务的增长还是会出现单表数量大导致查询效率下降的问题。

按照水平分库的思路，我们可以对单表进行水平拆分：

![水平分表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/水平分表.jpg)

**优化提升：**

1. 优化单一表数据量过大而产生的性能问题
2. 避免IO争抢并减少锁表的几率



### 切分规则

#### Hash取模

![分库分表-Hash取模](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分库分表-Hash取模.png)

1. **优点**：经过 hash 取模之后，分到库和分到表中的数据，都是均衡的，所以，不会出现资源倾斜的问题
2. **缺点**：若后续遇到业务暴增，没有在我们预估范围内，则要涉及到数据迁移，那就需要重新hash , 迁移数据，修改路由等



#### Range划分

![分库分表-Range划分](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分库分表-Range划分.png)

简单说，就是把数据划分范围，挨个存储，存满一个再存另一个。

1. **优点**：不需要数据迁移，后续数据即时增长很多也没问题
2. **缺点**：数据倾斜严重，比如上图，很长一段时间，都会只用到 1 个库，几个表



#### 一致性Hash

![分库分表-一致性Hash](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分库分表-一致性Hash.png)

一致性 hash 环的节点一般按 2^32-1 来算，但是一般如果业务 ID 足够均衡，则可以降一些节点，如 4096 等等，4 个库的话，则均衡的分布在图上的位置，而数据通过 hash 计算，对应到外环的虚拟节点，然后归属于真实的库，对于表也可以同样处理。或者，直接把表节点部署在外环上，直接将数据归属于表。

1. **优点**：更加均匀，并且在需要扩容时，数据迁移的量级更小，只需要迁移 1/N 的数据即可
2. **缺点**：路由算法要复杂，但是对于能得到的好处，这点复杂度就可以忽略了



#### 地理区域划分

比如按照华东，华南，华北这样来区分业务，七牛云应该就是如此。



#### 时间范围划分

按照时间切分，就是将6个月前，甚至一年前的数据切出去放到另外的一张表，因为随着时间流逝，这些表的数据 被查询的概率变小，所以没必要和“热数据”放在一起，这个也是“冷热数据分离”。



## 面临问题

### 分布式事务问题

使用分布式事务中间件解决，具体是通过最终一致性还是强一致性分布式事务，看业务需求。



### 跨节点关联查询Join问题

切分之前，我们可以通过Join来完成。而切分之后，数据可能分布在不同的节点上，此时Join带来的问题就比较麻烦了，考虑到性能，尽量避免使用Join查询。解决这个问题的一些方法：

- **全局表**

  全局表，也可看做是 "**数据字典表**"，就是系统中所有模块都可能依赖的一些表，为了避免跨库Join查询，可以将 **这类表在每个数据库中都保存一份**。这些数据通常很少会进行修改，所以也不担心一致性的问题。

- **字段冗余**

  **利用空间换时间，为了性能而避免join查询**。例：订单表保存userId时候,也将userName冗余保存一份，这样查询订单详情时就不需要再去查询"买家user表"了。

- **数据组装**

  **在系统层面，分两次查询**。第一次查询的结果集中找出关联数据id，然后根据id发起第二次请求得到关联数据。最后将获得到的数据进行字段拼装。



### 跨节点分页、排序函数问题

跨节点多库进行查询时，会出现Limit分页、Order by排序等问题。分页需要按照指定字段进行排序，当排序字段就是分片字段时，通过分片规则就比较容易定位到指定的分片；当排序字段非分片字段时，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户。



### 全局主键避重问题

如果都用`主键自增`肯定不合理，如果用`UUID`那么无法做到根据主键排序，所以我们可以考虑通过`雪花ID`来作为数据库的主键，有关雪花ID可以参考我之前写的博客：静态内部类单例模式实现雪花算法。



### 数据迁移问题

采用`双写的方式`，修改代码，所有涉及到分库分表的表的增、删、改的代码，都要对新库进行增删改。同时，再有一个数据抽取服务，不断地从老库抽数据，往新库写，边写边按时间比较数据是不是最新的。



### 公共表

参数表、数据字典表等都是数据量较小，变动少的公共表，属于高频联合查询的依赖表。分库分表后，我们需要将这类表在每个数据库都保存一份，所有对公共表的更新操作都同时发送到所有分库执行。



## ShardingSphere

Apache ShardingSphere 是一套开源的分布式数据库解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用的产品组成。 它们均提供标准化的数据水平扩展、分布式事务和分布式治理等功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。

Apache ShardingSphere 旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 关系型数据库当今依然占有巨大市场份额，是企业核心系统的基石，未来也难于撼动，我们更加注重在原有基础上提供增量，而非颠覆。

Apache ShardingSphere 5.x 版本开始致力于可插拔架构，项目的功能组件能够灵活的以可插拔的方式进行扩展。 目前，数据分片、读写分离、数据加密、影子库压测等功能，以及对 MySQL、PostgreSQL、SQLServer、Oracle 等 SQL 与协议的支持，均通过插件的方式织入项目。 开发者能够像使用积木一样定制属于自己的独特系统。Apache ShardingSphere 目前已提供数十个 SPI 作为系统的扩展点，而且仍在不断增加中。

![ShardingSphere Scope](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/shardingsphere-scope_cn.png)

|            | ShardingSphere-JDBC | ShardingSphere-Proxy | ShardingSphere-Sidecar |
| :--------- | :------------------ | :------------------- | ---------------------- |
| 数据库     | 任意                | MySQL/PostgreSQL     | MySQL/PostgreSQL       |
| 连接消耗数 | 高                  | 低                   | 高                     |
| 异构语言   | 仅 Java             | 任意                 | 任意                   |
| 性能       | 损耗低              | 损耗略高             | 损耗低                 |
| 无中心化   | 是                  | 否                   | 是                     |
| 静态入口   | 无                  | 有                   | 无                     |



### ShardingSphere-JDBC

定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。

- 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC
- 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等
- 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库

![ShardingSphere-JDBC Architecture](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/shardingsphere-jdbc-brief.png)

1. 引入 maven 依赖

   ```xml
   <dependency>
       <groupId>org.apache.shardingsphere</groupId>
       <artifactId>shardingsphere-jdbc-core</artifactId>
       <version>${latest.release.version}</version>
   </dependency>
   ```

2. 规则配置

   ShardingSphere-JDBC 可以通过 `Java`，`YAML`，`Spring 命名空间`和 `Spring Boot Starter` 这 4 种方式进行配置，开发者可根据场景选择适合的配置方式。 详情请参见[配置手册](https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-jdbc/configuration/)。

3. 创建数据源

   通过 `ShardingSphereDataSourceFactory` 工厂和规则配置对象获取 `ShardingSphereDataSource`。 该对象实现自 JDBC 的标准 DataSource 接口，可用于原生 JDBC 开发，或使用 JPA, MyBatis 等 ORM 类库。

   ```java
   DataSource dataSource = ShardingSphereDataSourceFactory.createDataSource(dataSourceMap, configurations, properties);
   ```



### ShardingSphere-Proxy

定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL 版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端(如：MySQL Command Client, MySQL Workbench, Navicat 等)操作数据，对 DBA 更加友好。

- 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用
- 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端

![ShardingSphere-Proxy Architecture](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/shardingsphere-proxy-brief.png)

1. 规则配置

   - 编辑`%SHARDINGSPHERE_PROXY_HOME%/conf/config-xxx.yaml`。详情请参见[配置手册](https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-proxy/configuration/)
   - 编辑`%SHARDINGSPHERE_PROXY_HOME%/conf/server.yaml`。详情请参见[配置手册](https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-proxy/configuration/)

2. 引入依赖

   如果后端连接 PostgreSQL 数据库，不需要引入额外依赖。如果后端连接 MySQL 数据库，请下载 `mysql-connector-java-5.1.47.jar`，并将其放入 `%SHARDINGSPHERE_PROXY_HOME%/lib` 目录。

3. 启动服务

   - 使用默认配置项

     ```shell
     sh %SHARDINGSPHERE_PROXY_HOME%/bin/start.sh
     ```

     默认启动端口为 `3307`，默认配置文件目录为：`%SHARDINGSPHERE_PROXY_HOME%/conf/`。

   - 自定义端口和配置文件目录

     ```shell
     sh %SHARDINGSPHERE_PROXY_HOME%/bin/start.sh ${proxy_port} ${proxy_conf_directory}
     ```

4. 使用ShardingSphere-Proxy

   执行 MySQL 或 PostgreSQL的客户端命令直接操作 ShardingSphere-Proxy 即可。以 MySQL 举例：

   ```shell
   mysql -u${proxy_username} -p${proxy_password} -h${proxy_host} -P${proxy_port}
   ```



### ShardingSphere-Sidecar(TODO)

定位为 Kubernetes 的云原生数据库代理，以 Sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的啮合层，即 `Database Mesh`，又可称数据库网格。

Database Mesh 的关注重点在于如何将分布式的数据访问应用与数据库有机串联起来，它更加关注的是交互，是将杂乱无章的应用与数据库之间的交互进行有效地梳理。 使用 Database Mesh，访问数据库的应用和数据库终将形成一个巨大的网格体系，应用和数据库只需在网格体系中对号入座即可，它们都是被啮合层所治理的对象。

![ShardingSphere-Sidecar Architecture](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/shardingsphere-sidecar-brief.png)

1. 规则配置

   编辑`%SHARDINGSPHERE_SCALING_HOME%/conf/server.yaml`。详情请参见[使用手册](https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-scaling/usage/)。

2. 引入依赖

   如果后端连接 PostgreSQL 数据库，不需要引入额外依赖。如果后端连接 MySQL 数据库，请下载 `mysql-connector-java-5.1.47.jar`，并将其放入 `%SHARDINGSPHERE_SCALING_HOME%/lib` 目录。

3. 启动服务

   ```shell
   sh %SHARDINGSPHERE_SCALING_HOME%/bin/start.sh
   ```

4. 任务管理

   通过相应的 HTTP 接口管理迁移任务。详情参见[使用手册](https://shardingsphere.apache.org/document/current/cn/user-manual/shardingsphere-scaling/usage/)。



### 混合架构

ShardingSphere-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP 应用；ShardingSphere-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP 应用以及对分片数据库进行管理和运维的场景。

Apache ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 ShardingSphere-JDBC 和 ShardingSphere-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。

![ShardingSphere Hybrid Architecture](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/shardingsphere-hybrid.png)

**功能列表**

- 数据分片
  - 分库 & 分表
  - 读写分离
  - 分片策略定制化
  - 无中心化分布式主键

- 分布式事务
  - 标准化事务接口
  - XA 强一致事务
  - 柔性事务

- 数据库治理
  - 分布式治理
  - 弹性伸缩
  - 可视化链路追踪
  - 数据加密

