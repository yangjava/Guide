## 服务稳定性保障思路

一、稳定性的定义

```armasm
量化平台的稳定性通常有两种方式，首先是平台的可用性，其次是线上问题和线上故障。

可用性一般等于（年度总时间 - 网站不可用时间） / 年度总时间
可用性的标准通常以几个九来衡量，比如四个九，即 99.99%的可用性，即平台全年的不可用时间不能超过0.01% * 365 * 24 * 60 = 52.56分钟。这是一个严峻的挑战。

线上故障通常指大规模地影响到了平台服务的质量甚至是可用性，通常分为P1/P2/P3/P4四个等级，一般称为P1/P2为重大故障。故障不一定会影响可用性，可用性有影响一定是有故障发生。

线上问题是指平台服务的小问题，小Bug，没有故障那么严重，但一定程度可以反映平台的稳定性。

稳定性的目标通常是 杜绝一切故障隐患，在上线前保证问题的发现与解决，若故障难以避免，要求有能快速发现并报警的方法，且有快速处理解决故障的能力。
```

二、稳定性保障的思路

2.1 核心链路梳理

```undefined
梳理出产品中真正核心业务模块，对整个调用链路进行分析，如，是否为强依赖，是否需要降级和限流侧率，资源是否满足极限要求等。
通常来说，当某个服务不可用时，业务链路不能继续进行且直接影响到业务功能，则为强依赖，否则为弱依赖。在进行强弱依赖治理时，要去除不合理的依赖，尽可能将非必须的强依赖降低为弱依赖。
对于强依赖要保障，合理分析制定降级、限流策略，缓存、资源调优。
对于弱依赖要隔离，故障时及时进行限流、超时拦截设置关停业务，以免对核心服务造成影响。   
```

2.2 监控能力

```bash
监控反应的是能够尽可能快地发现线上故障从而快速解决，以此来降低损失。
监控通常包括以下方面：
    系统监控：机器资源状态 （内存、CPU） 、应用性能指标状态（qps/latency）、Core异常监控、日志监控
    业务监控：业务指标大盘、业务服务监控
对于业务监控，监控的频率通常和监控目标的重要程度有关，必要时也要按一定周期（日/周）进行汇总统计。
```

2.3 性能摸底、资源调优

```undefined
性能摸底是对现上服务服务的当前的极限能力就行探测，合理规划机器资源，对于资源过甚的业务要减少机器预算，对于触及资源红线的要进行扩容。
性能摸底压测流程可以例行化，一来可以及时发现业务迭代过程中可能带来性能问题，二来是时刻进行资源预警和分配。
限流摸底方案参考《一种性能资源摸底的方案》
```

2.4. 限流降级

```mipsasm
限流是根据某个应用或基础部件的某些核心指标，如QPS 或并发线程数，来决定是否将后续的请求进行拦截。限流可以牺牲少部分用户的体验从而保障大部分用户的产品体验。
降级是通过判断某个应用或组件的服务状态是否正常，来决定是否继续提供服务（服务降级）
降级从策略上又可以分为多级：功能降级（抵御短时峰值陡增）、Cache降级（抵御长时峰值增长）削峰限流（抵御长时超高峰值）、极端容灾（抵御极端服务宕机灾情）
```

2.4.预案措施

```undefined
预案是在有意识的为潜在或有可能出现的风险制定应对处置方案，涉及 事前预防、事中救援、事后处理三个风险阶段，又从容错、容量两个维度进行考量。容量分别是对服务上、下限保障的处理措施，如在超上限时进行限流，在遇到故障的恶劣情况下需要保证服务的最低质量，容错又分为无损、有损。无损不会对用户的体验造成损失。比如，事先准备预案链路、准备扩容机房。有损，会牺牲全部/部分用户的全部/部分体验，限流和降级都是有损的。
```

2.6 故障处理

```mipsasm
故障处理机制的目的是为了缩短故障时间，时间越短，用户影响就越小。
因此需要事先准备好各种故障下的处理流程：服务回滚流程、服务开关管理、cache管理、限流降级开关管理。
```

三、保障体系
综合以上点 稳定性保障体系框图如下：
![img](https://images2018.cnblogs.com/blog/592634/201808/592634-20180807083231618-564502954.png)

稳定性治理，主要包括可用性和容错性治理工作。业界没有统一的定义，有把可用性和稳定性并列的。

稳定，肯定首先要确保少出故障，然后是故障出现后可以快速发现，快速响应、快速恢复或者减少影响。

少出故障，离不可代码质量控制工作和高可用架构设计工作。代码质量控制工作，手段主要有代码审查、代码静态扫描(sonar工具）等；高可用性架构主要手段无非是容灾性设计，以及复制、冗余、主备设计，包括负载均衡。

出现故障后快速发现和响应，要求自动化的监控和告警系统；如何快速恢复和减少影响，则主要依靠容错性设计。容错性设计，主要包括故障隔离（物理和逻辑的分层隔离，包括熔断、舱壁手段）、故障转移(failover）、快速失败(failfast)、let it crash、失败回退（failback)。也包括超时控制、并发控制、连接数控制等流量控制手段，优雅降级手段。

稳定性治理，除可用性和容错性设计外，还包括对依赖管理，减少对依赖系统强依赖，尽量弱依赖（为降级提供条件）、异步化（方便链路优化）。微服务使服务粒度减小，链路拉长，需要分布式服务跟踪系统来生成依赖链路图。

随着系统的不断架构调整、功能增加，以及业务发展带来的流量变化，系统能不能有效应对，必须进行容量规划和压测验证。容量规划，需要为扩容和缩容提供依据，压测验证系统是否满足服务SLA保证，这两项工作都是为了确保系统稳定运行在合理区间中。单系统的压测不能保证整个链路没有问题，因此分布式全链路压测同样非常重要。

稳定性治理工作，本质上是要系统首先活下来，然后是活的好，因此也要基于成本收益进行考量。

稳定性治理工作，就是要为失败进行设计(Design for failure)，容灾和容错是主要手段。可用率、容错率是主要指标。而所有的工作是否有效，需要故障演练验证，除了人工模拟故障外，还需要想Chaos Monkey系统来提高可靠性。

稳定性治理工作，是大促保障的核心内容。一个集容量规划、分布式服务治理系统、分布式服务跟踪系统、分布式日志系统、自动化监控和告警系统、分布式全链路压测系统、Chaos Money 故障演练系统为稳定性治理提供了绝佳的保证。
系统稳定性治理最佳实践

稳定压倒一切，没有稳定就没有生成。国家是如此，业务系统也是如此。老子说，“治大国若烹小鲜”，治理系统也是要做到同样，要掌握火候，精选食材，用料恰当，辅以煎炒烹炸煮，则方能出一盘好菜。

很多同学优先考虑公务员、教师等职业，其中非常重要的一个因素就是这类职业相对稳定。应用系统稳定性也是如此，是所有因素里的前提。

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/pz9bc7zjsw.jpeg?imageView2/2/w/1620)

试想一下，支付宝经常打不开，你还敢存钱进去吗？

试想一下，微信经常卡死，你会不会烦躁想骂人？

试想一下，京东一支付就系统繁忙，你慌不慌？

那么系统稳定性该如何治理？有没有什么标准或者可以放之四海皆准的方法论和实践？

# 系统稳定性问题

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/yce9txvb4l.jpeg?imageView2/2/w/1620)

一个系统稳定性取决于很多因素，同样也受制于很多因素。

为什么丰田可以40万公里不修，而某些国产车开了1年，除了喇叭不响，其它地方都会响？

为什么纳智捷不停车邮箱根本加不满？

类似的，一个系统的稳定性也受制于很多方面，总结下来大致是以下几点

## 硬件及网络影响

这个是指应用的容器层面的影响，包括服务硬件、机房、网络带宽等。这类因素往往对于开发人员来说不可控，比如管道工人挖断光缆、运营商网络抖动或延迟等。

这类问题一般来说很难预期，也很难针对问题进行专项治理。覆盖全世界网络，根本没有人可以保证网络绝对的稳定性，也根本无法保障一台服务器永远不宕机。

## 高并发大流量

对于一般的系统来说，流量往往不会成为瓶颈，一般的中小企业做的都是垂直类、行业类业务，总体覆盖人群一般不会很多，百万级已经是天大的调用量了，大部分的系统估计只有几十到几百的QPS。针对初级的访问流量，尚且不用做高并发大流量的设计，遵循普通的开发准则即可覆盖99%以上的业务了。

但是针对全球型、全国型的国民级应用，则高并发和大流量是主要要考虑的一个TOPIC，不仅在技术方案上就做重点设计规划，还需要持续关注和治理专项。

比如微信DAU达到了10亿级别，这个量级对所有相关的系统都提出了很大的挑战。一个全员推送不当，可能导致全国人民微信抖三抖。

比如淘宝双十一高峰期需要处理66W笔订单，这个量级是一般的系统根本无法满足的。

## 方案设计缺陷

方案缺陷指技术方案或者产品逻辑设计有问题，在满足一定的条件时，可能不会出问题，但是走了某个流程或者流量到了某个层次就凸显异常问题。

## 编码缺陷

这个是最多的一类问题，开发者由于水平层次不一，有的停留在“能运行就赢了”，有的停留在“能在各种入参变化的情况下运行正常”，有的会“极端流量网络异常下依然可以运行”，从“面向正确”到“面向错误”编程。《致命Bug：软件缺陷的灾难与启示》的一书有很多例子，都是因为编码BUG，导致了超乎想象的灾难。比如第一章提到的“0.000000095的误差夺走28条生命”等故障都历历在目。

阿里云也发生过

编码错误类型比较多，通常有以下一些：

- 边界值和异常值未考虑

这是最常见的一个问题，大多数初级开发者，往往仅考虑正确且正常的输入的情况下代码合计，而往往忽略边界条件。比如对入参不做非空校验、价格不做负数校验、不做超过列表长度的取数校验等。这类问题看似很小，但是在极端情况下，往往会引起很多莫名低级的BUG。

还有一类是数据源本身本身就可能是出错的，代码没有对这类数据做处理，则导致在运行期间发生异常。接口设计里有一个很简单的原则“外部输入数据是不可信的”。即在考虑接口设计时，就要充分考虑外部数据的一切可能性，有可能传递数字格式的String类型，偏偏就传递了一个字符串；预期传递一个null的，偏偏传递了一个字符串"null"等。

- 并发异常

这类错误相对来说更为“高端”“隐藏”，在单机的条件下一般不会出现问题，但是到了生产多线程环境可能就会出来。由于在开发过程中，对多线程的测试和调试也相对较少，导致开发和测试期间都不能简单的复现。比如用户加入组织后，组织又立即解散了，在并发的情况下，可能组织先解散，然后再出现用户加入组织的事件，如果不考虑其它因素，代码处理可能就会产生BUG。

- 幂等或事务异常

对需要做唯一性约束的数据没有做约束，比如订单ID不能重复、用户名不能重复等等。很多情况需要对数据一致性要严格保障。其中重要的两个就是幂等和事务，这两个点发生了异常，就会导致很多不一致性，进而产生大量脏数据和错误数据。

# 稳定性治理

稳定性治理的核心三板斧，监控、压测和演练。

## 监控

监控如果做到了360无死角，则可以第一时间主动发现系统异常，定位到了解决则是相对明确的。那么稳定性自然也有很高的保障，可以说监控是稳定性保障的前提。

那么如何做监控？做哪些监控？这个是每个工程师要细致思考的问题。监控不仅仅是发一条[短信](https://cloud.tencent.com/product/sms?from=10680)出来，告知工程师“系统异常”，而是应该快速告知工程师，哪个系统的哪个接口出现了什么样的异常，越具体，就能够帮助工程师快速定位问题，也就给工程师止血和恢复提供了更快的可能。

监控的告警方式一般有短信、电话、邮件，或者可以使用钉钉通知、钉钉机器人等方式。

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/2rp4k6d2ec.jpeg?imageView2/2/w/1620)

钉钉群机器人接入方式也比较简单，还是非常推荐大家使用的，可以把多个告警源接入到钉钉，统一管理。

### [应用性能监控](https://cloud.tencent.com/product/tapm?from=10680)

#### load监控

load监控是对于容器和应用层面的监控，可以监控整个容器水位消耗，对业务负载能够起一个快速的决策。比如正常情况下。

简单来说，对于单核CPU，load为1表示已经满载了，如果超过1意味着好负荷了，有任务已经无法直接占用CPU资源需要一直等待，系统也可能即将崩溃。同理，对于多核CPU，load则对应具体的核数。一般来说4核CPU的话，load为4表示满载。正常情况下，70%的负荷率是比较正常的，即单核CPU的Load要小于等于0.7。

> load详解文章： https://www.ruanyifeng.com/blog/2011/07/linux_load_average_explained.html

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/k19uwzspge.jpeg?imageView2/2/w/1620)

如上图所示，用TOP命令即可快速查看当前机器的load，图中的0.88则表示当前机器的实际负载，对于4核CPU来说，负载算比较轻的。

#### java线程数

java中线程是宝贵的资源，各种代码运行后都是以线程的方式来运转，线程是生产实例的最小单位，因此监控线程数就可以监控java应用的实际运行情况。相对load来说，java线程更纯粹，load包含了整个CPU的所有消耗，包括其他系统内置的消耗，而java线程则是统计了实际为应用本身提供服务的情况。

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/6qthvpg1gi.jpeg?imageView2/2/w/1620)

特别在多线程的情况下，可以快速发现是否有线程泄露等问题，这类问题往往通过其他指标无法直接观看。比如发现一个接口的RT已经接近超时，机器load飙升，但是接口本身又没有发现问题，这时候可以看看线程的行为。

#### GC监控

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/xy91v6apao.jpeg?imageView2/2/w/1620)

> GC是垃圾收集的意思（GarbageCollection）,内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。

简单来说，JAVA运行期间的数据是以对象的方式存在的，而对象又存在生命周期，在时刻运行期间会产生大量的对象占用内存，因此需要有垃圾对象回收的策略，称之为GC。

GC监控可以判断编码中的对象内存泄露，可以对JVM本身进行，也可以对不合理的编码方式进行监控优化。

### 业务指标监控

#### 数据大盘

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/xefsa8z9lk.jpeg?imageView2/2/w/1620)

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/afes8z0fqj.jpeg?imageView2/2/w/1620)

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/ndpz1tmzfw.jpeg?imageView2/2/w/1620)

> 诸如阿里云相关的云服务都提供了大盘的功能，可以监控机器的流量水位等。一些中小公司可以使用开源的工具搭建自身内部系统的监控。 具体可查看链接： https://blog.csdn.net/smooth00/article/details/85001823

大盘可以根据业务情况区分，比如订单创建大盘、用户登录大盘、账单支付成功大盘等，大盘可以帮助从全局角度观察同比环比趋势。通过趋势图可以对波峰、波谷、毛刺进行分析定位问题和优化系统。比如在突然的流量暴涨和暴跌，可以在数据大盘图快速定位。

#### 成功率

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/0k1v9fmqpy.jpeg?imageView2/2/w/1620)

又称之为“接口健康度”，及在指定时间内成功请求/总请求的比率，这个是接口稳定性里最重要的指标，可以通过这个指标直接判定系统的可用性。

健康度可以根据自己业务目标设置告警等级，比如某些非核心功能，可能设置低于70%成功率告警，对于某些核心功能，比如IM的会话送达率，则可能设置99.99%以上。

#### RT值

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/m3henyp26w.jpeg?imageView2/2/w/1620)

接口重要的几个指标，除了QPS外，就是RT了，RT表示一个接口的响应能力，RT越短说明接口提供服务的效率越高，相反，RT越大则代表提供服务能力越弱，一个系统内部出现多种情况的故障时，一般伴随的就对外提供能力变弱，响应的就是RT会明显加长。

因此通过监控RT可以很好的监控一个系统的提供服务能力的强弱，不仅在故障期间能够更为直观，在平常也可以给工程师提供一个性能优化的视图。

#### 主日志

除了几个常规指标外，还有一个指标叫主日志，即把核心的错误日志都打印到同一个文件，通过关键的关键字进行错误分类，进而实现对错误分类和错误详情进行告警，帮助快速处理和解决线上问题，实现免登机器定位错误的能力。

### DB监控

相对应用来说，DB往往是整个系统的性能瓶颈。优化一个准则，也是尽量把流量挡在DB外。也很容易理解，代码或硬件问题，可以通过集群和分布式来解决，但是所有写流量基本上都要集中在一个[数据库](https://cloud.tencent.com/solution/database?from=10680)实例中。因此数据库crash了往往会造成整体功能不可用。

当然在方案设计时候，可以显示的优化DB层面的设计，比如读写分离、数据缓存、分库分表等。这个对数据库的设计在其他内容中介绍，本文还是关注如何做好DB监控。

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/n212alwj2g.jpeg?imageView2/2/w/1620)

#### QPS

> qps 每秒处理的查询数

QPS和TPS是判断整体数据库的核心指标，该指标也表示了当前的数据库运行能力，当然应用层不好判定具体暴涨业务的时候，可以通过数据库的SQL来对数据进行定位，辅助定位到异常的SQL语句，进而分析故障原因。

#### TPS

> tps 每秒处理的事务数

#### 慢SQL

数据查询变慢的一个原因，就是慢SQL，慢SQL顾名思义，就是一条执行很慢的SQL，主要是查询慢，一般来说SQL查询都是毫秒级别，到了秒级的话，就是基本慢SQL了。慢SQL严重性不仅是本身查询慢，而是慢SQL会大量消耗数据库本身的资源，比如数据库的线程、CPU、内存等，从而使得整体服务能力下降，最终拖垮整个数据库。

## 压测

相对运行期间监控来说，压测是运行前的一个提前发现问题的操作。压测也需要做到常规化、持续化执行。压测可以用自动化的手段来在真实环境下获得系统的稳定性问题，提前发现系统异常和薄弱环节。

像大型的双十一，就会用提前压测的方法模拟双十一甚至超过几倍的请求，来进行全链路探测系统问题，并及时进行修复解决，这样当然就不怕双十一期间的自然大流量完全压垮系统。也提前做到了心中有数。

![img](https://ask.qcloudimg.com/http-save/yehe-1410546/ukqnz2elxq.jpeg?imageView2/2/w/1620)

> 压测一般用我们上述提及的指标作为衡量标准，比如load是否到1，成功率是否下跌，主日志是否出现错误等等。

## 演练

监控发现问题治理，压测探查系统薄弱瓶颈，而演练则是在生产上真实的创建故障，用来发现系统稳定性、鲁棒性和自动恢复性，还能检测应用负责人是否有快速响应系统异常的能力、止血和修复的能力。

演练类似消防演习，即人工的采取某些措施，比如断网、停DB、MOCK缓存失败等。

# 总结

稳定性治理三板斧：监控、压测和演练。应用容器三要素：load、线程、GC。业务数据监控四要素：大盘、成功率、RT值、主日志。DB监控三要素：QPS、TPS、慢SQL。

系统稳定性压倒一切，只有保障了好了稳定性，才能帮助业务蓬勃增长，因此稳定性治理始终是工程师基本能力之一。

最近一直在忙618大促的全链路压测&稳定性保障相关工作，结果618还未开始，生产环境就出了几次生产故障，且大多都是和系统稳定性、性能相关的bad case。

生产全链路压测终于告一段落，抽出时间将个人收集的稳定性相关资料整理review了一遍，顺带从不同的维度，谈谈稳定性相关的“务虚”认知和思考。。。

 

***\*一、SLA!\****

在开始谈稳定性保障之前，我们先来聊聊业内经常提及的一个Topic：**SLA！**

业内喜欢用SLA （服务等级协议，全称：service level agreement）来衡量系统的稳定性，对互联网公司来说就是网站服务可用性的一个保证。

9越多代表全年服务可用时间越长服务越可靠，停机时间越短。就以一个标准99.99%为例，停机时间52.6分钟，平均到每周也就是只能有差不多1分钟的停机时间，

也就是说网络抖动这个时间可能就没了。保证一个系统四个9或者更高的五个9，需要一套全体共识严格标准的规章制度，没有规矩不成方圆。创建的规范有如下几种：

1、研发规范、自身稳定；

2、事务中不能包含远程调用；

3、超时时间和重试次数要合理；

4、表数据操作必须double check，合理利用索引，避免出现慢查询、分库分表不走分表键；

5、没有有效的资源隔离， 避免不同业务共用一个线程池或连接池；

6、合理的系统拓扑，禁止不合理的服务依赖，能去依赖就去依赖，否则同步依赖尽量改成异步弱依赖；

7、精简的代码逻辑；

8、核心路径流程必须进行资源隔离，确保任何突发情况主流程不能受影响。

 

**二、单服务稳定性**

**关键字：开关可控、单一职责、服务隔离、异常兜底、监控发现！**

对于稳定性来说，抛开整体系统架构设计，单就每个业务域服务的稳定性也是非常的重要。

只有每个业务环节都稳如泰山，才可以保障整个稳定性。单服务的稳定可以从以下几个方面来进行：

**1、禁用设计**：应该提供控制具体功能是否开启可用的配置，在相应的功能服务出现故障时，快速下线局部功能，以保证整体服务的可用性；

**2、必要的缓存**：缓存是解决并发的利器，可以有效的提高系统的吞吐量。按照业务以及技术的纬度必要时可以增加多级缓存来保证其命中率；

**3、接口无状态性**：服务接口应该是无状态的，当前接口访问不应该依赖上层接口的状态逻辑；

**4、接口单一职责性**：对于核心功能的接口，不应该过多的耦合不属于它的功能。如果一个接口做的事情太多应做拆分，保证单接口的稳定性和快速响应；

**5、第三方服务隔离性**：任何依赖于第三方的服务（不论接口还是中间件等），都应该做到熔断和降级，不能有强耦合的依赖；

**6、业务场景兜底方案**：核心业务场景需要做到完整的兜底方法，从前端到后端都应该有兜底措施；

**7、服务监控与及时响应**：每个服务应该做好对应的监控工作，如有异常应及时响应，不应累积。

 

**三、集群稳定性**

**关键字：系统架构、部署发布、限流熔断、监控体系、压测机制！**

对于集群维度的稳定性来说，稳定性保障会更加复杂。单服务是局部，集群是全局。一个见微知著，一个高瞻远瞩。

**1、合理的系统架构**：合理的系统架构是稳定的基石；

**2、小心的代码逻辑**：代码时刻都要小心，多担心一点这里会不会有性能问题，那里会不会出现并发，代码就不会有多少问题；

**3、优秀的集群部署**：一台机器永远会有性能瓶颈，优秀的集群部署，可以将一台机器的稳定放大无限倍，是高并发与大流量的保障；

**4、科学的限流熔断**：高并发来临时，科学的限流和熔断是系统稳定的必要条件；

**5、精细的监控体系**：没有监控体系，你永远不会知道你的系统到底有多少隐藏的问题和坑，也很难知道瓶颈在哪里；

**6、强悍的压测机制**：压测是高并发稳定性的试金石，能提前预知高并发来临时，系统应该出现的模样；

**7、胆小的开发人员**：永远需要一群胆小的程序员，他们讨厌bug，害怕error，不放过每一个波动，不信任所有的依赖。

 

**四、稳定性专项**

专项指的是**针对某些特定场景下的特定问题而梳理出对应的方案**。下面是针对一些常见的稳定性专项的概述：

**1、预案**：分为定时预案和紧急预案，定时预案是大促常规操作对于一系列开关的编排，紧急预案是应对突发情况的特殊处理，都依赖于事前梳理；

**2、预热**：分为JIT代码预热和数据预热，阿里内部有专门的一个产品负责这块，通过存储线上的常态化流量或者热点流量进行回放来提前预热，

　 起源于某年双十一零点的毛刺问题，原因是访问了数据库的冷数据rt增高导致的一系列上层限流，现在预热已经成了大促之前的一个必要流程。

**3、强弱依赖**:梳理强弱依赖是一个偏人肉的过程，但是非常重要，这是一个系统自查识别潜在风险点并为后续整理开关限流预案和根因分析的一个重要参考，

　 阿里内部有一个强弱依赖检测的平台，通过对测试用例注入RPC调用的延迟或异常来观察链路的依赖变化，自动梳理出强弱依赖关系。

**4、限流降级熔断**:应对突发流量防止请求超出自身处理能力系统被击垮的必要手段；

**5、监控告警&链路追踪**:监控分为业务监控、系统监控和中间件监控和基础监控，作为线上问题发现和排查工具，重要性不言而喻。

 

**五、稳定性建设**

稳定性建设，就和基础技术建设一样，是一个**长期迭代和不断调整的过程**，业内常见的稳定性建设类型，主要有如下几种：

**1、容量规划**：个人感觉容量规划在大厂里也并没有做的很好，更多依赖的是业务方自己拍脑袋，然后全链路压测期间验证，不够就再加机器。

**2、混沌工程**：混沌工程是近几年比较火的名词，通过不断给系统找麻烦来验证并完善系统能力，阿里在这块花了很大的精力建设红蓝军对抗攻防，进行定期和不定期的演练，

　 最后以打分的形式来给各个部门系统做排名，除了系统层面的故障演练外还有资金演练，篡改线上sql语句制造资损来测试业务监控纠错的能力，通过制造小错来避免大错。

　 跳转门：[混沌工程-初识](https://www.cnblogs.com/imyalost/p/12271620.html)

**3、流量调度**：通过metric秒级监控和聚类算法实时找出异常单机来降低RPC流量权重，提升集群整体吞吐能力减少异常请求。

**4、容灾&异地多活**：起源于15年某施工队将光纤挖断带来的支付宝故障，由此出来的三地五中心和单元化架构，异地多活本身的成本比较高，

　 然后又存在数据同步的延时问题和切流带来的脏数据问题，对于业务和技术都有比较高的要求。常见的容灾有如下几种：

　 1）缓存挂掉，集群重启缓存预热如何处理？本地缓存，多级缓存是否可以替代？

　 2）分布式锁，是否有开关一键切换？比如：ZK/ETCD编写的分布式锁；

　 3）大促峰值流量，如何防止外部ddos攻击？如何识别流量类型？

　 4）资源隔离：资源隔离，服务分组，流量隔离；

　 5）高可用思想：避免单点设计！

　 6）容错：容错上游，防御下游。容错主要需要注意如下几点：

　 　 6-1：外部依赖的地方都要做熔断，避免雪崩；

　　  6-2：对于依赖我们的上游要限流，防止上游突发流量超过自己系统能够扛住的最大QPS；

　　  6-3：对于下游既要评估好接口超时时间，防止下游接口超时异常导致自己系统被拖累；

　　  6-4：下游的接口要考虑各种异常情况，需要考虑中间状态，通过引入柔性事务，确保数据最终一致。

**5、异地多活**

**异地多活的本质，是数据中心架构的演进**。

**1）演进**：单机房——双机房——异地灾备——异地多活；

**2）定义**：分多个地域、多个数据中心运行线上的业务，并且每个IDC均提供在线服务；

**3）优点**：弹性扩展能力、流量就近接入、灵活调度、提升可用性与用户体验、容灾；

**4）步骤**：

　 4-1：基础设施：机房之间专线互联，保证网络质量稳定；

　 4-2：持久存储：一主三从，主IDC同步复制，异地IDC异步复制；

　 4-3：中间件：DB、MQ、分布式存储；

　 4-4：应用部署：根据应用域划分，不同应用部署在不同地域，保持亲缘性；

　 4-5：流量接入与调度：网络协议兼容，DNS，动态调度用户就近访问；

　 4-6：监控与运维保障：专线实时监控，确保发生故障时可以触发Failover（失效备援）和流量调度。

 

**六、稳定性思考**

**关键字：阶段工作、角色转变！**

稳定性建设是一个演进的阶段性过程，主要分为三个阶段：

**1、发现问题解决问题**：当问题较多时候就很被动，很多时候我们通过不断完善监控来确保我们来快速定位问题，但仍处于被动的一方；

**2、主动寻找问题**：混沌工程、破坏性测试、极限压测、红蓝对抗等手段，一方作为创造问题方不断挑战系统极限，另一方见招拆招快速修复。

**3、角色转变：**这个过程中会积累很多处理问题的经验，不断完善系统健壮性，争取在用户发现问题前消灭于萌芽中。角色转变，变被动为主动。

 

**七、推荐阅读**

聊聊服务灾备

大促稳定性建设

运维监控体系建设

这样的高可用，我不要

高并发限流，到底限的什么鬼

新浪微博平台稳定性体系介绍

StabilityGuide—稳定大于一切

没有预热，不叫高并发，叫并发高

信号量限流，高并发限流不得不说的秘密

# 网易七鱼服务治理实践 

2021-03-25 10:46

众所周知，业务架构是逐渐演进的。随着业务和组织的发展，架构在持续变化，而这种变化往往会体现在业务域的划分上。动态调整是一个过程，一般是先拆开再治理。简单的拆分会引入依赖和耦合的问题，本文将着重讨论业务架构演进过程中出现的服务和模块边界问题以及解决这些问题的实践。

# 1. 业务架构演进

由于七鱼本身复杂度较大，因此在系统设计之初就是微服务架构，这个架构随着组织和业务的发展出现了比较重大的变化。

这些变化从根本上说，是系统拆分从“按照功能拆分”，逐渐演进到“按照业务领域”进行拆分的过程。

- **按照功能拆分**

早期，由于大家都是一个团队，每个人负责的内容是按照功能来划定的。这种做法简单、直观而且且符合“单一职则”原则。

同时，由于采用面向数据和过程编程的方式（即需要什么数据自己负责组装，公共逻辑采用Jar包共享的方式进行提取和复用），因此服务和服务之间耦合度不高。

单一职责、高内聚低耦合，在业务发展初期支撑了七鱼以极高的速度进行版本和功能迭代

![img](https://p3.itc.cn/images01/20210325/a464a773a989493d8cd31e8c6e76722b.png)

- **按照业务拆分**

随着业务的不断发展，七鱼逐步形成了几个大的独立售卖业务线，以及一些相对小但独立性也很强的支撑性业务。

到了这个阶段，可以明显的感觉到早期按照功能划分的服务不再能适应组织发展的需要了。最典型的情况就是各个业务组在开发功能的时候都会去改基础服务域的服务。

此时，“单一职责”原则虽然得到了保存，但是“高内聚、低耦合”原则被破坏掉了。这导致大量的代码耦合、不合理的服务依赖、发布依赖。这些问题会影响线上系统稳定性、维护性，拖慢研发效能。

为了解决上述问题，我们开始了七鱼服务治理项目。

![img](https://p1.itc.cn/images01/20210325/d782c0c21b8a4ac18c194ba58dad638c.png)

# 2. 服务分级

在识别耦合和依赖的合理性之前，我们需要对服务进行分级。没有分级，就没有服务优先级、依赖倒置等技术优化的切入点，也不会有资源和进度的安排依据。

核心模块非核心模块的这种说法是由来已久的。按照这个思路可以对服务的重要程度度进行分级。

在七鱼中我们对服务的层级做了如下定义：（注意，这里不包括中间件和数据库等基础设施）

- P0: 系统级基础服务，如果宕机则导致大面积可感知的服务异常，通常数量很少（底层共用数据的管理和查询等）
- P1: 核心业务基础服务和核心功能，如果出现了宕机则某个核心业务出现主流程不可用
- P2: 非核心业务应用和核心业务非核心功能（数据报表、系统通知等）
- P3: 内部支撑业务（运营后台、运维后台等）

在定义了服务分级之后，我们有如下的一些基本原则：

- 下层服务不能直接调用上层服务
- 下层服务稳定性可用性不能受限于上层服务
- 同层级服务之间尽可能保持逻辑隔离
- 底层服务只提供基础能力，并保持模型稳定

# **3. 边界问题和解法**

在服务分级以及模块划分的基础上，我们在日常开发中识别了如下问题：

**代码耦合**：由于我们经历了从功能拆分到业务拆分的过程。在中间阶段，某些服务承载了多个业务域的业务。虽然拆分之后这些服务被划到了某个业务组，但是代码的耦合依然存在，Owner需要按照其他组的需求修改代码。代码耦合会导致如下问题：

1. Owner无法完全掌控自己的代码和计划，响应其他组的需求可能打乱自己的时间安排；
2. Owner无法排期时，为了赶时间而让非Owner来进行开发，由于熟悉程度不够，从而引起问题；
3. 上线存在依赖，这又会引入发布权限和发布顺序的问题。

**不合理依赖**：又分成了反向依赖、环状依赖、强弱依赖三个方面

1. 反向依赖：底层服务依赖上层服务。调用倒置，造成下层服务稳定性受上层服务影响。
2. 环状依赖：A依赖B，B依赖A，当然可能是有中间服务的 A->B->C->A 这样的依赖关系。会导致发布顺序失控。
3. 不合理的强弱依赖：业务上弱依赖的但是服务调用上是强依赖的。即业务上，某个服务宕机应该不影响核心功能，但是实际结果是该服务宕机之后核心功能不可用。

在七鱼的基础服务边界治理过程中，采用了如下的一些技术手段来优化服务。为了优化某个场景，可能会联合采用多个手段来共同完成目标。

![img](https://p3.itc.cn/images01/20210325/d6b5a0a827924135bba2da3ca53f9427.png)

下面我们就从场景出发，对这些技术手段做简单的介绍。

# **4. 边界治理实践**

**拆分**

拆分分为下面几种情况：

- 无共用代码的，一般是各业务方比较独立的功能，直接拆走即可；
- 有共用代码的，又分成两种情况：共用部分属于基础能力、共用部分属于业务逻辑的一部分：
- 共用的基础能力可以单出抽取成Jar包或者抽取成独立的服务
- 而如果业务逻辑耦合
- 如果底层模型无法拆开，这里就透露出了业务领域划分存在问题；
- 而如果为了展示的需要，则可以作为聚合服务存在，本身不影响业务领域划分；

早期，所有的页面接口承载在同一个服务中，导致该应用不得不被归类为P0级。其中大部分接口都是业务内部设置和数据查看，因此属于无共用代码的情况，可以直接拆走。

![img](https://p0.itc.cn/images01/20210325/73a760b710be4404a4796f91c4a6e899.png)

此外，所有页面都依赖于一系列基础数据，如企业信息、客服信息、权限信息等。这属于为了展示的需求而全局依赖某种基础能力的情况，因此我们将页面基础数据查询功能独立为一个单独服务。由于所有页面依然依赖这些数据，所以这个服务还是P0。

这样的拆分操作，将原来大杂烩的P0拆成了一个功能单一、逻辑简单、代码稳定的P0，以及一系列P1和P2服务。

**按需加载 + 弱依赖降级**

对于依赖多业务方的场景，通常这些依赖有强弱之分。

- 弱依赖：A场景依赖B服务，但是A跟B并非强相关。即如果B不可用，A的主流程还可以运行。
- 强依赖：A依赖B，且A场景跟B强相关。即如果B不可用，A的主流程走不通。

对于强依赖，必须要保证可用性，同时要做到按需加载以尽可能减少不必要的风险。弱依赖是允许出现不可用，但是为了防止弱依赖不可用之后出现不友好的提示，需要提供降级方案。

上个例子中页面依赖的所有基础数据，在拆分前是一起加载的。一项数据加载失败就可能导致所有数据返回失败。虽然业务上各个基础数据不一定都能用到，但是事实上就是强依赖了所有基础数据。

但是由于大量的数据是共用的，因此为每个页面单独写一个数据封装接口又是非常不划算的。所以我们在新数据加载服务中，引入GraphQL来解决这个问题。

![img](https://p0.itc.cn/images01/20210325/d6de4fa6c2cb4c3cb7e9d15a00a93b92.png)

![img](https://p4.itc.cn/images01/20210325/bc846bbe5f4e493299d0d258f867a6c8.png)

GraphQL要求将数据拆分成基础的单元，通过组装query语句来向Server查询，query语句既包含了原子数据项，又包含了最终想要的数据格式。

相较于为每个页面写一个单独的数据接口，以满足按需加载的需要，这样做的好处很多：

- 原子数据的查询复用
- 按需加载
- 数据格式灵活可调
- 扩展容易
- 提供了丰富的数据运算和拼装能能力
- 跨前端技术栈

这里不对GraphQL做过多展开，感兴趣可以参考https://graphql.org/。

降级则通常有Hystrix或者Sentinel来完成。这里也不做过多的展开。

**边界变更**

业务领域往往有多种划分方式，但是有时候最符合业务领域的划分方式不一定是最现实可靠的划分方式。

从代码维护性和线上稳定角度考虑，有时候必须要对边界做重新划分。这里有几个参考原则：

- 减少P0应用数量
- 模型稳定、调用量大、有全局影响的业务逻辑可以放在一起
- 调整之后模型边界需要有明确的业务含义以便于理解和维护，不能硬凑。

七鱼的“企业信息管理”和“订单与服务包”开始分处于两个服务。但是日常工作中发现：企业管理调用量大且模型稳定；订单逻辑复杂变更多，但大部分调用量很小，只有“服务包查询”调用量很大且模型稳定。

我们将“服务包查询”的功能迁移到“企业信息管理”中，从概念上将“企业信息管理”模块变更成了“企业运行时管理”。通过边界变更，我们将两个P0级服务拆成一个P0一个P1，同时也保证了复杂多变的业务不影响稳定的底层服务。

![img](https://p3.itc.cn/images01/20210325/5871248fe5214e8eae803eb2ad9b29c0.png)

**领域模型优化**

领域模型如果出现了对其他领域数据的耦合，那么代码也一定是强耦合的。但是只要能确定业务域划分没问题，则可以通过领域模型优化的方式来解除耦合。

通常的做法是用KV表存储其他领域的关联数据、用事件驱动异步更新这个KV表，这样当前的领域模型就可以不关注数据的业务含义。

不关心业务含义只是存储数据，则底层模型就可以做到通用化并保持稳定，将反向依赖和代码耦合彻底清除掉。

七鱼的User表中除了基础的用户信息，还保存了“最近和最后联系时间”等业务方的数据。显然，这种层面的耦合会导致User模型被污染。但是，业务功能上这些信息是展示User信息必须的。

![img](https://p7.itc.cn/images01/20210325/12da6b69559d4d74868d64d616684440.png)

考虑到现在User需要展示的是“最新联系时间”，那么后续就有可能要展示“最新工单时间”、“最新短信时间”等等。如果持续去适配需求改动代码，则就造成了代码耦合。

从模型层面上做调整，增加UserInfoExt表，用键值对的形式提供扩展信息的存储，业务系统通过主动更新K-V值的方式来更新数据。这样就保证了User模型层的稳定、调用关系的优化、代码层的完全解耦。

![img](https://p4.itc.cn/images01/20210325/f504a5ff6d41455f9ba310a9b4b0a949.png)

**能力上推**

接领域模型优化。

领域模型的优化成本很高，实际中不一定有资源来完成这种重构。特别是涉及到P0级应用的底层模型变更，风险往往很大。

在需要底层数据和上层数据关联展示的场景中。关联展示的逻辑可以不承载在底层模型上，而是将这部分组装的过程上推到上层业务系统中去，从而解除底层模型的数据耦合。

接上面的例子，由于User是全局最核心的服务之一，改造的风险很大，最后我们并没有采用领域模型优化的方案。而是在这里做能力上推，将P0级别的能力推到P1级别去。

User核心模型删除“最近最后联系时间”，获取信息的过程上推到User-Gateway服务中来完成。User-Gateway虽然是属于基础业务域，但仅负责提供页面需要的用户数据，宕机也不影响底层会话、工单等数据流，因此属于P1级别的服务。

![img](https://p9.itc.cn/images01/20210325/e868f67ef29341849625c35ba5f293f0.png)

**事件驱动**

当业务流程耦合了其他业务域的流程时。有两个可能性存在：

- 强依赖上层业务流程的结果；
- 不依赖上层业务的结果。

如果不依赖上层业务的结果，那么可以通过生命周期事件的方式，将流程和核心节点广播出去，让上层业务独立完成后续的流程。

为了保证生命周期事件能够被顺利消费并触发业务逻辑。需要在中间件层保证消息的触达和幂等性，同时假设在极端情况下出现了执行失败，则需要提供消息补偿执行的机制。

最初，七鱼的企业注册的流程简单的串行过程，中间任何一个设置没做完，企业都没法初始化完成从而导致企业注册失败。

这种失败其实是很不划算的，即便某个业务没有初始化好，也可以试用其他业务，不至于完全丢失一个潜在客户。

![img](https://p1.itc.cn/images01/20210325/b7508808e65e4bef801f6766d08f75e8.png)

我们通过对企业注册生命周期进行建模，将“企业创建”事件广播出来，用事件驱动来完成整个注册过程。这样做的好处在于：

- 新增业务的初始化不会影响现有的官网代码，从而解除代码耦合；
- 部分业务的初始化失败不会影响整体的注册流程，解除对单一业务的强依赖；
- 官网作为P1级别服务，直接调用的只有P0的企业和客服管理服务，因此不存在反向依赖。

**异步调用**

接事件驱动。

当业务流程中底层流程依赖上层业务的结果时，解决这种依赖的方式有两种：

- 改造领域模型以解除强依赖。虽然比较彻底，但是往往成本很大。
- 直接调用并依赖结果；这样就造成了调用的反向依赖。

异步调用是为了解决直接调用产生的代码耦合和反向依赖的问题。通过事件驱动的方式获取上层业务的结果，而不是直接调用并获取结果。跟普通消息驱动不同，异步调用依赖返回的结果；跟直接调用不同，不依赖被调用方的接口。

在七鱼，删除客服需要校验：是否有未完成的电话 、会话、工单等。由于删除客服是属于基本的客服管理，所以在P0级服务中。而为了校验业务信息，则必须调用P1级服务。

如果采用领域模型重构的方式，让业务层将“能否删除”告知给“客服管理”并随着业务流程实时更新，则可以解除该反向依赖。但是这种方式需要侵入各个业务方的核心流程中，且需要修改当前业务逻辑，成本过高。

![img](https://p3.itc.cn/images01/20210325/56226d32287346e088b5e853cd333390.png)

在七鱼，我们设计实现了一个异步调用的组件。假设删除客服关注A、B、C三个服务的结果，则ABC则向注册中心注册关注删除事件。每次删除过程都会拿到关注列表，然后广播删除消息，业务方收到消息之后将结果返回到注册中心。删除客服依赖注册中心的通知机制获取到结果并决定是否完成删除。

由于该过程是异步驱动的，所以会有一个Timeout等待过程。这里有两个模式，一个是强依赖模式，Timeout则操作失败；另外一种是弱依赖模式，Timeout依然可以操作成功。

这样做的好处在于：

- 可以解除掉代码层的耦合，假设新增需要校验的业务方D，在D服务上注册关注删除事件即可。
- 这个改造对现有业务逻辑和核心流程没有影响，变更范围很有限。
- 不是直接调用，因此不会出现反向依赖。

**防腐层**

接事件驱动和异步调用。

最理想的情况下，业务方能够响应核心系统的驱动事件，从而完成完整的业务流程。

但是实际上，第三方由于不受本团队的控制，开发排期不可控、开发动力不强。为了能持续推进本团队的优化更新，需要依赖业务方的代码全部封装在一起并从P0级服务中剥离出去，防止对核心模型和核心流程造成污染。

我们在做注册解耦的时候，需要业务方响应企业注册的生命周期事件。在做客服删除的解耦时，则需要业务方集成异步调用组件。

这就导致我们的开发依赖了其他业务团队。为此我们单独增加一个防腐层应用，将响应生命周期事件和集成异步掉调用组件的逻辑迁移过去，最后完成了改造。

这样做，我们的开发才能够按时顺利完成，同时跟业务系统的耦合被限制在一个单独的应用中限制了腐化的范围，后期业务方迁移也变得非常容易。

# **5. 总结**

拆分、按需加载、弱依赖降级、边界变更、事件驱动这些手段是最开始做治理时的切入点。随着治理的深入，很多问题不是简单的拆分和变更能解决的。只有通过领域模型改造，才能找到一种方法将服务完整解开来。

但是，模型改造的成本往往是很高的，现实操作中我们不得不采用防腐层、能力上推、异步调用等手段来确保改造能实际进行下去，而不是陷入到无穷无尽的排期、测试中。

在梳理好了边界关系之后，上层服务还是可能影响底层服务稳定性的。主要的场景在不受控的调用产生的系统压力。这属于熔断限流降级的范畴，这里就不展开做详细讨论了。