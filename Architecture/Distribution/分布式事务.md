# 分布式事务

**什么是分布式事务？**

分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。一个大的操作由N多的小的操作共同完成。而这些小的操作又分布在不同的服务上。针对于这些操作，要么全部成功执行，要么全部不执行 。



**TCC**和**可靠消息最终一致性方案**是在生产中最常用。一个要求强一致，一个要求最终一致。**TCC**用于强一致主要用于核心模块，例如交易/订单等。**最终一致方案**一般用于边缘模块例如库存，通过mq去通知，保证最终一致性，也可以业务解耦。



## 分布式理论

### 事务一致性

**可靠程度**：强一致性 > 顺序一致性 > 因果一致性 > 最终一致性 > 弱一致性。

- **强一致性/线性一致性（Linearizability）**：写操作完成后，要求任何读取操作都能读取到最新的值
- **顺序一致性（Sequential Consistency）**：不保证操作的全局时序，但保证每个客户端操作能按顺序被执行
- **因果一致性（Causal Consistency）**：只对并发访问中具有因果关系的操作保证顺序
- **最终一致性（Eventual Consistency）**：不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化
- **弱一致性（Weak Consistency）**：数据更新后，能容忍后续的访问只能访问到部分或者全部访问不到



### ACID特性

`ACID`注重一致性，是传统关系型数据库的设计思路。`ACID`是数据库（MySQL）事务正确执行所必须满足的四个特性：

- **Atomicity（原子性）**：事务中的操作要么都做，要么都不做

- **Consistency（一致性）**：系统必须始终处在强一致状态下

- **Isolation（隔离性）**：一个事务的执行不能被其它事务所干扰

- **Durability（持久性）**：一个已提交的事务对数据库中数据的改变是永久性的



### CAP理论

`CAP`理论的核心思想是任何基于网络的数据共享系统最多只能满足数据一致性（`Consistency`）、可用性（`Availability`）和网络分区容忍（`Partition Tolerance`）三个特性中的两个。

- **一致性（Consistency）**：在分布式系统中的所有数据备份，在同一时刻是否拥有同样的值

- **可用性（Availability）**：在集群中一部分节点故障后，集群整体还能响应客户端的读写请求

- **分区容错性（Partition Tolerance）**：系统中任意信息的丢失或失败不会影响系统的继续运作

![CAP](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/cap.png)

这三个特性只能满足其中两个，牺牲另一个。大部分系统也都是如此：

- **一般来说分布式集群都会保证P优先**。即集群部分节点坏死不影响整个集群的使用，然后再去追求C和A。因为如果放弃P，那不如就直接使用多个传统数据库了。事实上，很多微服务分库分表就是这个道理
- **如果追求强一致性，那么势必会导致可用性下降**。比如在Master-Slave的场景中，Master负责数据写入，然后分发给各个节点，所有节点都写入成功，才算写入，这样保证了强一致性，但是延迟也会随之增加，导致可用性降低



### BASE理论

`BASE`关注高可用性。`BASE`理论是对`CAP`理论的延伸，核心思想是即使无法做到强一致性(`Strong Consistency`，`CAP`的一致性就是强一致性)，但应用可以采用适合的方式达到最终一致性(`Eventual Consitency`)。`BASE`是分别代表：

- **基本可用(Basically Available)**：指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用

- **软状态( Soft State)**：指允许系统存在中间状态，而该中间状态不会影响系统整体可用性

- **最终一致性( Eventual Consistency)**：指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态

BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。



## 弱一致性算法

### Gossip协议

Gossip 协议，顾名思义，就像流言蜚语一样，利用一种随机、带有传染性的方式，将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。根据 Base 理论，如果你需要实现最终一致性，那么就可以通过 Gossip 协议实现这个目标。Gossip协议的核心一共是三块内容：直接邮寄（Direct Mail）、反熵（Anti-entropy）和谣言传播（RumRumor mongering）。

作为一种异步修复、实现最终一致性的协议，反熵在存储组件中应用广泛，比如 Dynamo、InfluxDB、Cassandra，在需要实现最终一致性时，如果节点都是已知的，一般优先考虑反熵。当集群节点是变化的，或者集群节点数比较多时，这时要采用谣言传播的方式，同步更新数据，实现最终一致。



#### 直接邮寄（Direct Mail）

所谓直接邮寄，就是直接发送更新数据，当数据发送失败时，将数据缓存下来，然后重传。比如下图中，节点 A 直接将更新数据发送给了节点 B、D：
![Gossip协议-直接邮寄](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Gossip协议-直接邮寄.png)

虽然直接邮寄实现起来比较容易，数据同步也很及时，但可能会因为缓存队列满了而丢数据。 也就是说，只采用直接邮寄是无法实现最终一致性的。



#### 反熵（Anti-entropy）

熵，在物理学中是用来度量体系的混乱程度。所以，反熵就是要消除混乱，Gossip协议通过反熵来异步修复节点之间的数据差异，实现最终一致性。反熵的实现，一共有推、拉、推拉三种。 集群中的节点，每隔一段时间就会随机选择某个其他节点，然后交换自己的已有数据来消除两者之间的差异。但是正因为反熵需要节点间两两交换比对数据，所以执行反熵时的通讯成本会很高，不建议在实际场景中频繁执行反熵，应该通过引入Checksum等机制，降低需要对比的数据量和通讯次数。

**推方式**

推方式，就是将自己的所有副本数据，推给对方，修复对方副本中的熵：
![Gossip协议-推方式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Gossip协议-推方式.png)



**拉方式**

拉方式，就是拉取对方的所有副本数据，修复自己副本中的熵：
![Gossip协议-拉方式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Gossip协议-拉方式.png)



**推拉方式**

推拉方式，就是同时修复自己副本和对方副本中的熵：
![Gossip协议-推拉方式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Gossip协议-推拉方式.png)

虽然反熵很实用，但是执行反熵时，相关的节点都是已知的，而且节点数量不能太多，如果是一个动态变化或节点数比较多的分布式环境，反熵就不适用了。那么当你面临这个情况要怎样实现最终一致性呢？答案就是谣言传播。



#### 谣言传播（Rumor mongering）

谣言传播，广泛地散播谣言，它指的是当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有的节点都存储了该新数据。比如下图中，节点 A 向节点 B、D 发送新数据，节点 B 收到新数据后，变成活跃节点，然后节点 B 向节点 C、D 发送新数据：
![Gossip协议-谣言传播](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Gossip协议-谣言传播.png)

谣言传播非常具有传染性，它适合动态变化的分布式系统。



### Quorum NWR算法

CAP理论中的一致性一般指的是强一致性，也就是说写操作完成后，任何后续访问都能读到更新后的值。而BASE理论中的一致性指的是最终一致性，也就是说写操作完成后， 任何后续访问可能会读到旧数据，但是整个分布式系统的数据最终会达到一致。那么，如果我们的系统现在是最终一致性模型，也就是AP模型，突然有一天因为业务需要，要临时保证节点间的数据强一致性，有没有办法临时做这样的改造呢？一种办法是重新开发一套系统，但显然成本太高了。另一种办法就是本章要介绍的Quorum NWR算法。通过 Quorum NWR，我们可以自定义一致性级别。



**Quorum NWR三要素**

Quorum NWR 中有三个要素：N、W、R，它们是 Quorum NWR 的核心内容，我们就是通过组合这三个要素，实现自定义一致性级别的。



Quorum NWR 是非常实用的一个算法，能有效弥补 AP 型系统缺乏强一致性的痛点，给业务提供了按需选择一致性级别的灵活度。很多开源框架都利用Quorum NWR实现自定义一致性级别，比如Elasticsearch，就支持“any、one、quorum、all”4 种写一致性级别。



#### N（副本数）

N 表示副本数，又叫做复制因子（Replication Factor）。也就是说，N 表示集群中同一份数据有多少个副本。 注意，副本数不等同于节点数。（如果读者对Elasticsearch或Kafka有了解，可以把副本理解成Replica Shard）。比如下图中， DATA-1 有 2 个副本，DATA-2 有 3 个副本，DATA-3 有 1 个副本：

![QuorumNWR算法-副本数](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/QuorumNWR算法-副本数.png)

在实现 Quorum NWR 的时候，我们需要实现自定义副本数的功能，比如，用户可以指定 DATA-1 具有 2 个副本，DATA-2 具有 3 个副本，就像上图中的样子。



#### W（ 写一致性级别 ）

W，又称写一致性级别（Write Consistency Level），表示对于客户端的一次写操作，只有成功完成 W 个副本的更新，才算写操作成功。以下图中的DATA-2为例，当它的W=2时，如果客户端对 DATA-2 执行写操作，必须完成它的2 个副本的更新，才算完成了写操作：
![QuorumNWR算法-写一致性级别](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/QuorumNWR算法-写一致性级别.png)



#### R（读一致性级别）

R，又称读一致性级别（Read Consistency Level），表示对于客户端的一次写操作，需要读 R 个副本，然后最终返回最新的那份数据。以下图中的DATA-2为例，当它的R=2时，如果客户端读取DATA-2的数据，需要读取它的2 个副本中的数据，然后返回最新的那份数据：

![QuorumNWR算法-读一致性级别](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/QuorumNWR算法-读一致性级别.png)

N、W、R 值的不同组合，会产生不同的一致性效果，具体来说，有这么两种效果：

- 当 W + R > N 的时候，对于客户端来讲，整个系统能保证强一致性，一定能返回更新后的那份数据
- 当 W + R <= N 的时候，对于客户端来讲，整个系统只能保证最终一致性，可能会返回旧数据

我这里以DATA-2为例解释下，为什么W+R>N时，一定可以读到最新的数据。首先，DATA-2的N=3，W=2，R=2，那么当写数据时，必然有2个节点要写成功；此时再读数据，即使读到了一个没有写过的节点，由于要读2个节点的值，另一个必然是写成功的节点，所以最终返回给客户端的还是最新的数据。



### PBFT算法

我在《共识问题》一章中提到过，共识算法一共可以分为两大类：拜占庭容错算法（Byzantine Fault Tolerance，BFT）和故障容错算法（Crash Fault Tolerance，CFT）。Leslie Lamport在论文中提出的口信消息解决方案就属于BFT，需要考虑恶意节点的篡改、攻击等问题。但是，口信消息解决方案在现实场景中很难落地。比如，它并不关心这个共识的结果是什么，这会出现一种情况：现在适合进攻，但将军们达成的共识却是撤退。另外，实际场景中，我们往往需要就提议的一系列值（而不是单值）在拜占庭错误发生的时候，也能被达成共识。那应该怎么做呢？一种方案就是本文要讲解的PBFT算法。 PBFT算法，是一种能在实际场景中落地的BFT算法，它在区块链中应用广泛。

Raft 算法完全不适应有人作恶的场景，但PBFT 算法能容忍 (n - 1)/3 个恶意节点 (也可以是故障节点)。另外，相比 PoW 算法，PBFT 的优点是不消耗算力，所以在日常实践中，PBFT 比较适用于相对“可信”的场景中，比如联盟链。

此外，虽然PBFT 算法相比口信消息方案已经有了很大的优化，将消息复杂度从 O(n ^ (f + 1)) 降低为 O(n ^ 2)，能在实际场景中落地并解决共识问题，但 PBFT 还是需要比较多的消息，比如在 13 节点集群中（f 为 4），一次共识协商需要 237 个消息，所以决定了 PBFT 算法适用于中小型分布式系统。



#### oral message的问题

要理解PBFT算法，首先必须要明白口信消息解决方案（A solution with oral message）到底存在哪些问题？这些问题都是后续众多BFT算法在努力改进和解决的，理解了这些问题，能帮助你更好地理解后来的拜占庭容错算法的思想（包括 PBFT 算法）。oral message方案存在一个致命的缺陷：当将军总数为n，叛将数为f时，算法需要递归协商 f+1 轮，消息复杂度为 O(n ^ (f + 1))，消息数量指数级暴增。你可以想象一下，如果叛将数为 64，消息数已经远远超过 int64 所能表示的了，这是无法想象的。



**PBFT算法流程**

PBFT 算法，通过签名（或消息认证码 MAC）约束恶意节点的行为，每个节点都可以通过验证消息签名确认消息的发送来源，一个节点无法伪造另外一个节点的消息。PBFT 算法采用了三阶段协议，基于大多数原则（2f + 1，f表示叛将数）实现共识。另外，与oral message不同的是，PBFT 算法实现的是一系列值的共识，而不是单值的共识。我们先来看看PBFT 算法的流程。为了方便演示，假设一共有A、B、C、D四个节点，那么根据Paxos算法的理论，最多允许存在一个恶意节点（(4-1)/3=1），我们假设B是恶意节点，现在客户端发起了一个提议值（进攻），希望被各节点达成共识：

![PBFT算法流程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PBFT算法流程.png)

在PBFT算法中，第一个接收到客户端请求的节点，将成为Leader节点，我们假设A节点首先接收了到请求。A接收到客户端请求之后，会执行三阶段协议（Three-phase protocol）。



#### 预准备阶段

首先，A进入预准备（Pre-prepare）阶段，构造包含作战指令的预准备消息，并广播给其他节点（B、C、D）：

![PBFT算法流程-预准备阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PBFT算法流程-预准备阶段.png)



#### 准备阶段

B、C、D收到消息后 ， 不能确认自己接收到指令和其他人的是否相同。比如，D是叛徒，D收到了 2 个指令，然后他给A发送的是其中一个指令，给B、C发送的是另一个指令，这样就会出现无法一致行动的情况。

所以， 接收到预准备消息之后，B、C、D会进入准备（Prepare）阶段，并分别广播包含指令的准备消息给其他节点。这里我们假设叛徒D想通过不发送消息，来干扰共识协商：

![PBFT算法流程-准备阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PBFT算法流程-准备阶段.png)



#### 提交阶段

然后，当某个节点收到 2f 个包含相同指令的准备消息后，会进入提交（Commit）阶段（这里的f 为叛徒数， 2f 包括自己）。

> 在这里，思考一个问题：这个时候节点（比如B）可以直接执行指令吗？答案还是不能，因为B不能确认A、C、D是否收到了 2f 个一致的包含相同指令的准备消息。也就是说，B这时无法确认A、C、D是否准备好了执行指令。

进入提交阶段后，各节点分别广播提交消息给其他节点，也就是告诉其他节点：“我已经准备好了，可以执行指令了”：

![PBFT算法流程-提交阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PBFT算法流程-提交阶段.png)



#### 响应

最后，当某个节点收到 2f + 1 个验证通过的提交消息后（其中 f 为叛徒数，包括自己），也就是说，大部分的节点们已经达成共识，这时可以执行指令了，那么该节点将执行客户端的指令，执行完毕后发送执行成功的消息给客户端。

![PBFT算法流程-响应](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PBFT算法流程-响应.png)

最后，当客户端收到 f+1 个相同的响应（Reply）消息时，说明各个节点已经就指令达成了共识，并执行了指令。

在PBFT算法中，共识是否达成，客户端是会做判断的，如果客户端在指定时间内未收到请求对应的 f + 1 相同响应，就认为集群出故障了，共识未达成，客户端会重新发送请求。PBFT 算法通过视图变更（View Change）的方式，来处理主节点作恶，当发现主节点在作恶时，会以“轮流上岗”方式，推举新的主节点。



### PoW算法

谈起比特币，大家至少都应该有所耳闻吧？比特币是基于区块链实现的，而区块链运行在Internet上，这就存在有人试图作恶的情况。之前提到的口信消息解决方案和PBFT算法，虽然能防止坏人作恶，但只能防止少数，也就是 (n-1)/3 个坏人 (其中 n 为节点数)。可由于很多区块链是在公网环境，可能有坏人不断增加节点数，轻松突破 (n - 1) / 3 的限制。解决上述问题的方法就是PoW算法。PoW算法通过工作量证明（Proof of Work）增加了坏人作恶的成本，以此防止坏人作恶。本章，我就来讲讲PoW算法的原理。

PoW算法，属于拜占庭容错算法中的一种，能容忍一定比例的作恶行为，所以它在相对开放的场景中应用广泛，比如公链、联盟链。而非拜占庭容错算法（比如 Raft）无法对作恶行为进行容错，主要用于封闭、绝对可信的场景中，比如私链、公司内网的 DevOps 环境。



#### 工作量证明

什么是工作量证明 (Proof Of Work，简称PoW) ？你可以这么理解：就是一份证明，用来确认你做过一定量的工作。比如，你的大学毕业证书就是一份工作量证明，证明你通过 4 年的努力完成了相关课程的学习。

那么，回到计算机世界，具体来说就是，客户端需要做一定难度的工作才能得出一个结果，验证方却很容易通过结果来检查出客户端是不是做了相应的工作。

比如小肖去Google面试，说自己的编程能力很强，那么她需要做一定难度的工作（比如做个算法题）。根据做题结果，面试官可以判断她是否适合这个岗位。这就是一个现实版的工作量证明。具体的工作量证明过程，就像下图中的样子：
![PoW算法-工作量证明](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PoW算法-工作量证明.png)

**哈希运算**

既然工作量证明是通过指定的结果，来证明自己做过了一定量的工作。那么在区块链的 PoW 算法中需要做哪些工作呢？答案是哈希运算。哈希运算的核心是哈希函数（Hash Function），也叫散列函数。就是说，你输入一个任意长度的字符串，哈希函数会计算出一个哈希值。比如，我们对任意长度字符串（比如"tutuxiao"）执行 SHA256 哈希运算，就会得到一个 32 字节的哈希值，就像下面的样子：

```java
$ echo -n "tutuxiao" | sha256sum
bb2f0f297fe9d3b8669b6b4cec3bff99b9de596c46af2e4c4a504cfe1372dc52
```

那么我们如何通过哈希运算来证明工作量呢？

举个例子，我们给出的工作量要求是：基于一个基本的字符串（比如"tutuxiao"），在这个字符串后面添加一个整数值，然后对变更后的字符串进行 SHA256 哈希运算，如果运算后得到的哈希值（16 进制形式）是以"0000"开头的，就验证通过；为了达到这个工作量证明的目标，我们需要不停地递增整数值，对得到的新字符串进行 SHA256 哈希运算。按照这个规则，我们需要经过 35024 次计算，才能找到恰好前 4 位为 0 的哈希值：

```java
"tutuxiao0" => 01f28c5df06ef0a575fd0e529be9a6f73b1290794762de014ec84182081e118e
"tutuxiao1" => a2567c06fdb5775cb1e3ce17b72754cf146fcc6da75c8f1d87d7ab6a1b8c4523
...
"tutuxiao35022" =>
8afc85049a9e92fe0b6c98b02b27c09fb869fbfe273d0ab84ad8c5ac17b8627e
"tutuxiao35023" =>
0000ec5927ba10ea45a6822dcc205050ae74ae1ad2d9d41e978e1ec9762dc404
```

通过上面这个示例可以看到，工作量证明就是通过执行哈希运算，经过一段时间的计算后，得到符合条件的哈希值。在实际场景中，我们可以根据场景特点，制定不同的规则，比如，你可以试试分别运行多少次，才能找到恰好前 3 位和前 5 位为 0 的哈希值。



#### 区块链

区块链也是通过 SHA256 执行哈希运算，通过计算出符合指定条件的哈希值，来证明工作量的。因为在区块链中，PoW 算法是基于区块链中的区块信息来进行哈希运算的。区块链的区块，是由区块头、区块体 2 部分组成的：

![PoW算法-区块链](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PoW算法-区块链.png)

- 区块头（Block Head）：区块头主要由上一个区块的哈希值、区块体的哈希值、4 字节的随机数（nonce）等组成的，共80 字节固定长度
- 区块体（Block Body）：区块包含的交易数据，其中的第一笔交易是 Coinbase 交易，这是一笔激励矿工的特殊交易

在区块链中，给出的工作量要求是：对区块头执行 SHA256 哈希运算，得到的结果再执行一个哈希运算，计算出的哈希值，只有小于目标值（target），才是有效的。计算出符合条件的哈希值后，矿工就会把这个信息广播给集群中所有其他节点，其他节点验证通过后，会将这个区块加入到自己的区块链中，最终形成一串区块链，就像下图的样子：

![PoW算法-区块链串](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PoW算法-区块链串.png)

从上面这种工作量证明要求可以看出，算力越强，系统大概率会越先计算出这个哈希值。这也就意味着， 攻击者能挖掘一条比原链更长的攻击链，并将攻击链向全网广播。而按照约定，节点将接受更长的链，也就是攻击链，丢弃原链。就像下图的样子：

![PoW算法-攻击链](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/PoW算法-攻击链.png)

这样的话，按照比特币的区块链约定——“最长链胜出，其它节点在这条链基础上扩展”，那么如果坏人们掌握了 51% 的算力，就通过优势算力实现对最长链的争夺，也就是发起 51% 攻击，实现双花（Double Spending）。

即使攻击者只有 30% 的算力，他也有可能连续计算出多个区块的哈希值，挖掘出更长的攻击链，发动攻击； 另外，即使攻击者拥有 51% 的算力，他也有可能半天无法计算出一个区块的哈希值，也就是攻击失败。也就是说，能否计算出符合条件的哈希值，有一定的概率性，但长久来看，攻击者攻击成功的概率等同于攻击者算力的权重。



## 强一致性算法

**两阶段提交协议**

两阶段提交系统具有完全的C（一致性），很糟糕的A（可用性），很糟糕的P（容错性）。
首先，两阶段提交协议保证了副本间是完全一致的，这也是协议的设计目的。再者，协议在一个节点出现异常时，就无法更新数据，其服务可用性较低。最后，一旦协调者与参与者之间网络分化，无法提供服务。



**Paxos和Raft算法**

Paxos 协议和Raft算法都是强一致性协议。Paxos只有两种情况下服务不可用：一是超过半数的 Proposer 异常，二是出现活锁。前者可以通过增加 Proposer 的个数来 降低由于 Proposer 异常影响服务的概率，后者本身发生的概率就极低。最后，只要能与超过半数的 Proposer 通信就可以完成协议流程，协议本身具有较好的容忍网络分区的能力。



### Paxos协议

二阶段提交还是三阶段提交都无法很好的解决分布式的一致性问题，直到Paxos算法的提出，Paxos协议由Leslie Lamport最早在1990年提出，目前已经成为应用最广的分布式一致性算法。Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。



**为什么在Paxos运行过程中，半数以内的Acceptor失效都能运行？**

- 如果半数以内的Acceptor失效时 还没确定最终的value，此时，所有Proposer会竞争 提案的权限，最终会有一个提案会 成功提交。之后，会有半过数的Acceptor以这个value提交成功
- 如果半数以内的Acceptor失效时 已确定最终的value，此时，所有Proposer提交前 必须以 最终的value 提交，此值也可以被获取，并不再修改



**如何产生唯一的编号呢？**
在《Paxos made simple》中提到的是让所有的Proposer都从不相交的数据集合中进行选择，例如系统有5个Proposer，则可为每一个Proposer分配一个标识j(0~4)，则每一个proposer每次提出决议的编号可以为5*i + j(i可以用来表示提出议案的次数)。



#### 核心思想

- 引入了多个Acceptor，单个Acceptor就类似2PC中协调者的单点问题，避免故障
- Proposer用更大ProposalID来抢占临时的访问权，可以对比2PC协议，防止其中一个Proposer崩溃宕机产生阻塞问题
- 保证一个N值，只有一个Proposer能进行到第二阶段运行，Proposer按照ProposalID递增的顺序依次运行
- 新ProposalID的proposer比如认同前面提交的Value值，递增的ProposalID的Value是一个继承关系



**容错要求**

- 半数以内的Acceptor失效、任意数量的Proposer 失效，都能运行
- 一旦value值被确定，即使 半数以内的Acceptor失效，此值也可以被获取，并不再修改



#### 节点角色

Paxos 协议中，有三类节点:

- **Proposer（提案者）**

  - Proposer 可以有多个，Proposer 提出议案(value)。所谓 value，在工程中可以是任何操作，例如“修改某个变量的值为某个值”、“设置当前 primary 为某个节点”等等。Paxos 协议中统一将这些操作抽象为 value。
  - 不同的 Proposer 可以提出不同的甚至矛盾的 value，例如某个 Proposer 提议“将变量 X 设置为 1”，另一个 Proposer 提议“将变量 X 设置为 2”，但对同一轮 Paxos 过程，最多只有一个 value 被批准。

- **Acceptor（批准者）**

  - Acceptor 有 N 个，Proposer 提出的 value 必须获得超过半数(N/2+1)的
  - Acceptor 批准后才能通过。Acceptor 之间完全对等独立

- **Learner（学习者）**

  - Learner 学习被批准的 value。所谓学习就是通过读取各个 Proposer 对 value 的选择结果，如果某个 value 被超过半数 Proposer 通过，则 Learner 学习到了这个 value

  - 这里类似 Quorum 议会机制，某个 value 需要获得 W=N/2 + 1 的 Acceptor 批准，Learner 需要至少读取 N/2+1 个 Accpetor，至多读取 N 个 Acceptor 的结果后，能学习到一个通过的 value



#### 选举过程

![Paxos选举过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Paxos选举过程.png)

- **Phase 1：准备阶段**

  - **P1a：Proposer 发送 Prepare请求**

    Proposer 生成全局唯一且递增的ProposalID，向 Paxos 集群的所有机器发送 Prepare请求，这里不携带value，只携带N即ProposalID 

  - **P1b：Acceptor 应答 Prepare**

    Acceptor 收到 Prepare请求后，判断收到的ProposalID是否比之前已响应的所有提案的N大。

    - 如果是
      - 在本地持久化 N，可记为Max_N
      - 回复请求，并带上已Accept的提案中N最大的value（若此时还没有已Accept的提案，则返回value为空）
      - 做出承诺：不会Accept任何小于Max_N的提案

    - 如果否，则不回复或回复Error

- **Phase 2：选举阶段**

  - **P2a：Proposer 发送 Accept**
    经过一段时间后，Proposer 收集到一些 Prepare 回复，有下列几种情况：
    - 回复数量 > 一半的Acceptor数量，且所有的回复的value都为空，则Porposer发出accept请求，并带上自己指定的value
    - 回复数量 > 一半的Acceptor数量，且有的回复value不为空，则Porposer发出accept请求，并带上回复中ProposalID最大的value(作为自己的提案内容)
    - 回复数量 ≤ 一半的Acceptor数量，则尝试更新生成更大的ProposalID，再转P1a执行
  - **P2b：Acceptor 应答 Accept**
    Accpetor 收到 Accpet请求 后，判断：
    - 收到的N >= Max_N (一般情况下是 等于)，则回复提交成功，并持久化N和value
    - 收到的N < Max_N，则不回复或者回复提交失败
  - **P2c：Proposer 统计投票**
    经过一段时间后，Proposer 收集到一些 Accept 回复提交成功，有几种情况：
    - 回复数量 > 一半的Acceptor数量，则表示提交value成功。此时，可以发一个广播给所有Proposer、Learner，通知它们已commit的value
    - 回复数量 <= 一半的Acceptor数量，则 尝试 更新生成更大的 ProposalID，再转P1a执行
    - 收到一条提交失败的回复，则尝试更新生成更大的 ProposalID，再转P1a执行



**最后，经过多轮投票后，达到的结果是：**

- 所有Proposer都 提交提案成功了，且提交的value是同一个value
- 过半数的 Acceptor都提交成功了，且提交的是 同一个value



#### 约束条件

Paxos 协议的几个约束：

- P1: 一个Acceptor必须接受(accept)第一次收到的提案
- P2a: 一旦一个具有value v的提案被批准(chosen)，那么之后任何Acceptor 再次接受(accept)的提案必须具有value v
- P2b: 一旦一个具有value v的提案被批准(chosen)，那么以后任何 Proposer 提出的提案必须具有value v
- P2c: 如果一个编号为n的提案具有value v，那么存在一个多数派，要么他们中所有人都没有接受(accept)编号小于n的任何提案，要么他们已经接受(accpet)的所有编号小于n的提案中编号最大的那个提案具有value v



每轮 Paxos 协议分为准备阶段和批准阶段，在这两个阶段 Proposer 和 Acceptor 有各自的处理流程。Proposer与Acceptor之间的交互主要有4类消息通信，如下图：

![Paxos协议Proposer与Acceptor交互流程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Paxos协议Proposer与Acceptor交互流程.png)

 这4类消息对应于paxos算法的两个阶段4个过程：

- Phase 1
  - proposer向网络内超过半数的acceptor发送prepare消息
  - acceptor正常情况下回复promise消息
- Phase 2
  - 在有足够多acceptor回复promise消息时，proposer发送accept消息
  - 正常情况下acceptor回复accepted消息



### Raft协议

**三种角色**

**Raft是一个用于管理日志一致性的协议**。它将分布式一致性分解为多个子问题：**Leader选举（Leader election）、日志复制（Log replication）、安全性（Safety）、日志压缩（Log compaction）等**。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。**Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选者**（Candidate）：

- **Leader（领导）**：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志
- **Follower（群众）**：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志
- **Candidate（候选人）**：Leader选举过程中的临时角色

三种状态的转换关系如下：

[![Raft的三种状态转换](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft的三种状态转换.png)](https://shuwoom.com/wp-content/uploads/2018/05/raft_change_user.png)

Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。Raft算法将时间分为一个个的**任期（term）**，每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。



**Term（任期）**

**Raft 算法将时间划分成为任意不同长度的任期（term）**。任期用连续的数字进行表示。每一个任期的开始都是一次选举（election），一个或多个候选人会试图成为领导人。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。**Raft 算法保证在给定的一个任期最多只有一个领导人**。



**RPC（通信）**

**Raft 算法中服务器节点之间通信使用远程过程调用（RPC）**，并且基本的一致性算法只需要两种类型的 RPC，为了在服务器之间传输快照增加了第三种 RPC。RPC有三种：

- **RequestVote RPC**：候选人在选举期间发起
- **AppendEntries RPC**：领导人发起的一种心跳机制，复制日志也在该命令中完成
- **InstallSnapshot RPC**: 领导者使用该RPC来发送快照给太落后的追随者



#### 选举流程

**① Leader选举过程**

**Raft 使用心跳（heartbeat）触发Leader选举**。当服务器启动时，初始化为Follower。**Leader**向所有**Followers**周期性发送**heartbeat**。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。每一个follower都有一个时钟，是一个随机的值，表示的是follower等待成为leader的时间，谁的时钟先跑完，则发起leader选举。Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC。结果有以下三种情况：

- **赢得了多数的选票，成功选举为Leader**
- 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader
- 没有服务器赢得多数的选票，Leader选举失败，等待选举时间**超时后发起下一次选举**

![Raft的Leader选举过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft的Leader选举过程.jpg)



**② Leader选举的限制**

**在Raft协议中，所有的日志条目都只会从Leader节点往Follower节点写入，且Leader节点上的日志只会增加，绝对不会删除或者覆盖**。这意味着Leader节点必须包含所有已经提交的日志，即能被选举为Leader的节点一定需要包含所有的已经提交的日志。因为日志只会从Leader向Follower传输，所以如果被选举出的Leader缺少已经Commit的日志，那么这些已经提交的日志就会丢失，显然这是不符合要求的。这就是Leader选举的限制：**能被选举成为Leader的节点，一定包含了所有已经提交的日志条目**。



#### 日志复制

日志复制的目的是为了保证数据一致性。

- **日志复制的过程**

  Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。

  - 客户端的每一个请求都包含被复制状态机执行的指令
  - leader把这个指令作为一条新的日志条目添加到日志中，然后并行发起 RPC 给其他的服务器，让他们复制这条信息
  - 假如这条日志被安全的复制，领导人就应用这条日志到自己的状态机中，并返回给客户端
  - 如果follower宕机或者运行缓慢或者丢包，leader会不断的重试，直到所有follower最终都复制了所有的日志条目

  ![Raft日志复制的过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft日志复制的过程.png)
  简而言之，leader选举的过程是：1、增加term号；2、给自己投票；3、重置选举超时计时器；4、发送请求投票的RPC给其它节点。

- **日志的组成**

  日志由有序编号（log index）的日志条目组成**。**每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。
  ![Raft日志的组成](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft日志的组成.png)

  上图显示，共有 8 条日志，提交了 7 条。提交的日志都将通过状态机持久化到磁盘中，防止宕机。

- **日志的一致性**

  ![Raft日志的一致性](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft日志的一致性.jpg)

  **① 日志复制的两条保证**

  - 如果不同日志中的两个条目有着**相同的索引和任期号，则它们所存储的命令是相同的**（原因：leader 最多在一个任期里的一个日志索引位置创建一条日志条目，日志条目在日志的位置从来不会改变）
  - 如果不同日志中的两个条目有着**相同的索引和任期号，则它们之前的所有条目都是完全一样的**（原因：**每次 RPC 发送附加日志时**，leader 会把这条日志条目的前面的**日志的下标和任期号一起发送给 follower**，如果 **follower 发现和自己的日志不匹配，那么就拒绝接受这条日志**，这个称之为**一致性检查**）

  **② 日志的不正常情况**

  一般情况下，Leader和Followers的日志保持一致，因此 AppendEntries 一致性检查通常不会失败。然而，Leader崩溃可能会导致日志不一致：**旧的Leader可能没有完全复制完日志中的所有条目**。

  下图阐述了一些Followers可能和新的Leader日志不同的情况。**一个Follower可能会丢失掉Leader上的一些条目，也有可能包含一些Leader没有的条目，也有可能两者都会发生**。丢失的或者多出来的条目可能会持续多个任期。
  ![Raft日志的不正常情况](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft日志的不正常情况.jpg)

  **③ 如何保证日志的正常复制**

  Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。

  具体的操作是：**Leader会从后往前试**，每次AppendEntries失败后尝试前一个日志条目，**直到成功找到每个Follower的日志一致位置点（基于上述的两条保证），然后向后逐条覆盖Followers在该位置之后的条目**。

  总结一下就是：**当 leader 和 follower 日志冲突的时候**，leader 将**校验 follower 最后一条日志是否和 leader 匹配**，如果不匹配，**将递减查询，直到匹配，匹配后，删除冲突的日志**。这样就实现了主从日志的一致性。

  

#### 安全性

Raft增加了如下两条限制以保证安全性：

- 拥有**最新的已提交的log entry的Follower才有资格成为leader**
- **Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志**，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）
  ![Raft安全性](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft安全性.jpg)



#### 日志压缩

在实际的系统中，**不能让日志无限增长**，否则**系统重启时需要花很长的时间进行回放**，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）。每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。

![Raft日志压缩](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Raft日志压缩.png)

**Snapshot中包含以下内容**：

- **日志元数据，最后一条已提交的 log entry的 log index和term**。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上
- **系统当前状态**



当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC。做snapshot既不要做的太频繁，否则**消耗磁盘带宽**， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。**推荐当日志达到某个固定的大小做一次snapshot**。做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。



#### 成员变更

- **常规处理成员变更存在的问题**

  可能存在这样的一个时间点，两个不同的领导者在同一个任期里都可以被选举成功（双主问题），一个是通过旧的配置，一个通过新的配置。简而言之，成员变更存在的问题是增加或者减少的成员太多了，导致旧成员组和新成员组没有交集，因此出现了双主。

- **解决方案之一阶段成员变更**

  Raft解决方法是每次成员变更只允许增加或删除一个成员（如果要变更多个成员，连续变更多次）。



#### 常见问题

**问题1：Raft分为哪几个部分？**

主要是分为leader选举、日志复制、日志压缩、成员变更等。



**问题2：Raft中任何节点都可以发起选举吗？**

Raft发起选举的情况有如下几种：

- 刚启动时，所有节点都是follower，这个时候发起选举，选出一个leader
- 当leader挂掉后，**时钟最先跑完的follower发起重新选举操作**，选出一个新的leader
- 成员变更的时候会发起选举操作



**问题3：Raft中选举中给候选人投票的前提？**

**Raft确保新当选的Leader包含所有已提交（集群中大多数成员中已提交）的日志条目**。这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的**last log entry的term_id和index**，follower在接收到RequestVoteRPC消息时，**如果发现自己的日志比RPC中的更新，就拒绝投票**。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。



**问题4：Raft网络分区下的数据一致性怎么解决？**

发生了网络分区或者网络通信故障，**使得Leader不能访问大多数Follwer了，那么Leader只能正常更新它能访问的那些Follower，而大多数的Follower因为没有了Leader，他们重新选出一个Leader**，然后这个 Leader来接受客户端的请求，如果客户端要求其添加新的日志，这个新的Leader会通知大多数Follower。如果这时网络故障修复 了，那么原先的Leader就变成Follower，在失联阶段这个老Leader的任何更新都不能算commit，都回滚，接受新的Leader的新的更新（递减查询匹配日志）。



**问题5：Raft数据一致性如何实现？**

主要是通过日志复制实现数据一致性，leader将请求指令作为一条新的日志条目添加到日志中，然后发起RPC 给所有的follower，进行日志复制，进而同步数据。



**问题6：Raft的日志有什么特点？**

日志由有序编号（log index）的日志条目组成，每个日志条目包含它被创建时的任期号（term）和用于状态机执行的命令。



**问题7：Raft和Paxos的区别和优缺点？**

- Raft的leader有限制，**拥有最新日志的节点才能成为leader**，multi-paxos中对成为Leader的限制比较低，**任何节点都可以成为leader**
- Raft中Leader在每一个任期都有Term号



**问题8：Raft里面怎么保证数据被commit，leader宕机了会怎样，之前的没提交的数据会怎样？**

leader会通过RPC向follower发出日志复制，等待所有的follower复制完成，这个过程是阻塞的**。**老的leader里面没提交的数据会回滚，然后同步新leader的数据。



**问题9：Raft日志压缩是怎么实现的？增加或删除节点呢？？**

在实际的系统中，**不能让日志无限增长**，否则**系统重启时需要花很长的时间进行回放**，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃（以前的数据已经落盘了）**。**snapshot里面主要记录的是日志元数据，即最后一条已提交的 log entry的 log index和term。



### ZAB协议

ZAB 协议全称Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。Zookeeper 是一个为分布式应用提供高效且可靠的分布式协调服务。在解决分布式一致性方面，Zookeeper 并没有使用 Paxos ，而是采用了 ZAB 协议。

ZAB 协议定义：**ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持 `崩溃恢复` 和 `原子广播` 协议**。下面我们会重点讲这两个东西。基于该协议，Zookeeper 实现了一种 `主备模式` 的系统架构来保持集群中各个副本之间`数据一致性`。具体如下图所示：

![ZAB协议](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/ZAB协议.png)

上图显示了 Zookeeper 如何处理集群中的数据。所有客户端写入数据都是写入到 主进程（称为 Leader）中，然后，由 Leader 复制到备份进程（称为 Follower）中。从而保证数据一致性。从设计上看，和 Raft 类似。

那么复制过程又是如何的呢？复制过程类似 2PC，ZAB 只需要 Follower 有一半以上返回 Ack 信息就可以执行提交，大大减小了同步阻塞。也提高了可用性。



#### 消息广播

ZAB 协议的消息广播过程使用的是一个原子广播协议，类似一个 **二阶段提交过程**。对于客户端发送的写请求，全部由 Leader 接收，Leader 将请求封装成一个事务 Proposal，将其发送给所有 Follwer ，然后，根据所有 Follwer 的反馈，如果超过半数成功响应，则执行 commit 操作（先提交自己，再发送 commit 给所有 Follwer）。整个广播流程分为 3 步骤：

**第一步**：将数据都复制到 Follwer 中

![ZAB消息广播数据复制至Follwer](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/ZAB消息广播数据复制至Follwer.png)

**第二步**：等待 Follwer 回应 Ack，最低超过半数即成功

![ZAB消息广播等待Follwer回应ACK](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/ZAB消息广播等待Follwer回应ACK.png)

**第三步**：当超过半数成功回应，则执行 commit ，同时提交自己

![ZAB消息广播Commit](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/ZAB消息广播Commit.png)

通过以上 3 个步骤，就能够保持集群之间数据的一致性。实际上，在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，避免同步，实现异步解耦。还有一些细节：

- Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为事务ID（ZXID），ZAB 兮协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理
- 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞
- zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理
- 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）



#### 崩溃恢复

当Leader崩溃，即进入我们开头所说的崩溃恢复模式（崩溃即：Leader失去与过半Follwer的联系）。ZAB定义了2个原则：

- **ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交**
- **ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务**

所以，ZAB 设计了下面这样一个选举算法：**能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务。**针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID 最大）的事务，那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。而且这么做有一个好处是：**可以省去 Leader 服务器检查事务的提交和丢弃工作的这一步操作。**



#### 数据同步

当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader 服务器首先确认事务是否都已经被过半的 Follwer 提交了，即是否完成了数据同步。目的是为了保持数据一致。当所有的 Follwer 服务器都成功同步之后，Leader 会将这些服务器加入到可用服务器列表中。



**ZXID 生成**

在 ZAB 协议的事务编号 ZXID 设计中，ZXID 是一个 64 位的数字，其中低 32 位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader 都会产生一个新的事务 Proposal 并对该计数器进行 + 1 操作。而高 32 位则代表了 Leader服务器上取出本地日志中最大事务Proposal的ZXID，并从该ZXID中解析出对应的epoch值，然后再对这个值加一。

![ZAB数据同步ZXID](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/ZAB数据同步ZXID.png)

高 32 位代表了每代 Leader 的唯一性，低 32 代表了每代 Leader 中事务的唯一性。同时，也能让 Follwer 通过高 32 位识别不同的 Leader。简化了数据恢复流程。基于这样的策略：当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步。



## 两阶段提交/XA(2PC)

**核心思路**

参与者将操作成功或失败结果通知协调者，再由协调者根据所有参与者反馈情况决定参与者是否要提交操作还是中止操作。

![二阶段提交协议](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/2pc.png)

熟悉MySQL的同学对两阶段提交应该颇为熟悉，MySQL的事务就是通过**「日志系统」** 来完成两阶段提交的。两阶段协议可以用于单机集中式系统，由事务管理器协调多个资源管理器；也可以用于分布式系统，**「由一个全局的事务管理器协调各个子系统的局部事务管理器完成两阶段提交」** 。



### 第一阶段：投票阶段

![2PC第一阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/2PC第一阶段.jpg)

这个协议有 **「两个角色」** ，A节点是事务的协调者，B和C是事务的参与者。事务的提交分成两个阶段：

- 第一个阶段是 **「投票阶段」**
  - 协调者首先将命令 **「写入日志」**
  - **「发一个prepare命令」** 给B和C节点这两个参与者
  - B和C收到消息后，根据自己的实际情况，**「判断自己的实际情况是否可以提交」**
  - 将处理结果 **「记录到日志」** 系统
  - 将结果 **「返回」** 给协调者



###  第二阶段：决定阶段

![2PC第二阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/2PC第二阶段.jpg)

- 第二个阶段是 **「决定阶段」**

  当A节点收到B和C参与者所有的确认消息后

  - **「判断」** 所有协调者 **「是否都可以提交」**
  - 如果可以则 **「写入日志」** 并且发起commit命令
  - 有一个不可以则 **「写入日志」** 并且发起abort命令
  - 参与者收到协调者发起的命令，**「执行命令」**
  - 将执行命令及结果 **「写入日志」**
  - **「返回结果」** 给协调者



### 两阶段提交缺点

- **单点故障**：一旦事务管理器出现故障，整个系统不可用
- **数据不一致**：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致
- **响应时间较长**：整个消息链路是串行的，要等待响应结果，不适合高并发的场景
- **不确定性**：当事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功



### 无法解决的问题

当协调者和参与者同时出现故障时，两阶段提交无法保证事务的完整性。如果调者在发出commit消息之后宕机，而唯一接收到commit消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，因为没人知道事务是否已经被提交。



## 三阶段提交(3PC)

3PC针对2PC做了改进：

- **引入超时机制**：在2PC中，只有协调者拥有超时机制，3PC同时在协调者和参与者中都引入超时机制
- **在2PC的第一阶段和第二阶段中插入一个准备阶段**：保证了在最后提交阶段之前各参与节点的状态是一致的

![三阶段提交协议](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/3pc.png)

### 第一阶段：CanCommit

协调者向参与者发送事务执行请求CanCommit，参与者如果可以提交就返回YES响应，否则就返回NO响应。



### 第二阶段：PreCommit

协调者根据参与者反馈的结果来决定是否继续执行事务的PreCommit操作，根据协调者反馈的结果，有以下两种可能：

- 假如协调者收到参与者的反馈结果都是YES，那么就会执行PreCommit操作
  - **发送预提交请求**：协调者向参与者发送PreCommit请求，并进入Prepared阶段
  - **事务预提交**：参与者接收到PreCommit请求后，执行事务操作
  - **响应反馈**：事务操作执行成功，则返回ACK响应，然后等待协调者的下一步通知
- 假如有任何一个参与者向协调者发送了NO响应，或者等待超时之后，协调者没有收到参与者的响应，那么就中断事务
  - **发送中断请求**：协调者向所有参与者发送中断请求
  - **中断事务**：参与者收到中断请求之后（或超时之后，仍未收到协调者的请求），执行事务中断操作



### 第三阶段：DoCommit

- **执行提交**
  - **发送提交请求**：协调者收到ACK之后，向所有的参与者发送DoCommit请求
  - **事务提交**：参与者收到DoCommit请求之后，提交事务
  - **响应反馈**：事务提交之后，向协调者发送ACK响应
  - **完成事务**：协调者收到ACK响应之后，完成事务
- **中断事务**
  - 在第二阶段中，协调者没有收到参与者发送的ACK响应，那么就会执行中断事务



## 补偿机制(TCC)

两阶段提交（2PC）和三阶段提交（3PC）并不适用于并发量大的业务场景。TCC事务机制相比于2PC、3PC，不会锁定整个资源，而是通过引入补偿机制，将资源转换为业务逻辑形式，锁的粒度变小。**核心思想**：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。TCC分为三个阶段：

- **Try**：这个阶段对各个服务的资源做检测以及对资源进行锁定或者预留
- **Confirm** ：执行真正的业务操作，不作任何业务检查，只使用Try阶段预留的业务资源，Confirm操作要求具备幂等设计，Confirm失败后需要进行重试
- **Cancel**：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，即执行回滚操作，释放Try阶段预留的业务资源 ，Cancel操作要求具备幂等设计，Cancel失败后需要进行重试
  ![Try-Confirm-Cancel](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Try-Confirm-Cancel.png)

TCC 事务机制相比于上面介绍的2PC，解决了其几个缺点：

- **协调者单点**。由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群
- **同步阻塞**。引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小
- **数据一致性**。有了补偿机制之后，由业务活动管理器控制一致性

总之，TCC 就是通过代码人为实现了两阶段提交，不同的业务场景所写的代码都不一样，并且很大程度的**增加**了业务代码的**复杂度**，因此，这种模式并不能很好地被复用。


**TCC案例场景**

TCC将一次事务操作分为三个阶段：Try、Confirm、Cancel，我们通过一个订单/库存的示例来理解。假设我们的分布式系统一共包含4个服务：订单服务、库存服务、积分服务、仓储服务，每个服务有自己的数据库，如下图：
![TCC案例场景-订单服务](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC案例场景-订单服务.png)

从正常流程上讲，TCC仍是一个两阶段提交协议。但在执行出现问题的时候，有一定的自我修复能力，如果任何一个事务参与者出现了问题，协调者可以通过执行逆操作来取消之前的操作，达到最终的一致状态（比如冲正交易、查询交易）。从TCC的执行流程也可以看出，服务提供方需要提供额外的补偿逻辑，那么原来一个服务接口，引入TCC后可能要改造成3种逻辑：

- **Try**：先是服务调用链路依次执行Try逻辑
- **Confirm**：如果都正常的话，TCC分布式事务框架推进执行Confirm逻辑，完成整个事务
- **Cancel**：如果某个服务的Try逻辑有问题，TCC分布式事务框架感知到之后就会推进执行各个服务的Cancel逻辑，撤销之前执行的各种操作

> 注意：在设计TCC事务时，接口的Cancel和Confirm操作都必须满足幂等设计。



### Try

Try阶段一般用于锁定某个资源，设置一个预备状态或冻结部分数据。对于示例中的每一个服务，Try阶段所做的工作如下：

- 订单服务：先置一个中间状态“UPDATING”，而不是直接设置“支付成功”状态
- 库存服务：先用一个冻结库存字段保存冻结库存数，而不是直接扣掉库存
- 积分服务：预增加会员积分
- 仓储服务：创建销售出库单，但状态是UNKONWN

![TCC-Try](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC-Try.png)



### Confirm

根据Try阶段的执行情况，Confirm分为两种情况：

- **理想情况下，所有Try全部执行成功，则执行各个服务的Confirm逻辑**
- **部分服务Try执行失败，则执行第三阶段——Cancel**

Confirm阶段一般需要各个服务自己实现Confirm逻辑：

- 订单服务：confirm逻辑可以是将订单的中间状态变更为PAYED-支付成功
- 库存服务：将冻结库存数清零，同时扣减掉真正的库存
- 积分服务：将预增加积分清零，同时增加真实会员积分
- 仓储服务：修改销售出库单的状态为已创建-CREATED
  ![TCC-Confirm](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC-Confirm.png)

> Confirm阶段的各个服务本身可能出现问题，这时候一般就需要TCC框架了（比如ByteTCC，tcc-transaction，himly），TCC事务框架一般会记录一些分布式事务的活动日志，保存事务运行的各个阶段和状态，从而保证整个分布式事务的最终一致性。



### Cancel

如果Try阶段执行异常，就会执行Cancel阶段。比如：对于订单服务，可以实现的一种Cancel逻辑就是：将订单的状态设置为“CANCELED”；对于库存服务，Cancel逻辑就是：将冻结库存扣减掉，加回到可销售库存里去。
![TCC-Cancel](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC-Cancel.png)

> 许多公司为了简化TCC使用，通常会将一个服务的某个核心接口拆成两个，如库存服务的扣减库存接口，拆成两个子接口：①扣减接口 ②回滚扣减库存接口，由TCC框架来保证当某个接口执行失败后去执行对应的rollback接口。



## 可靠消息最终一致性方案

该方案其实就是在分布式系统当中，把一个业务操作转换成一个消息，然后利用消息来实现事务的最终一致性。

> 比如从A账户向B账户转账的操作，当服务A从A账户扣除完金额后，通过消息中间件向服务B发一个消息，服务B收到这条消息后，进行B账户的金额增加操作。

可靠消息最终一致性方案一般有两种实现方式，原理其实是一样的：

- **基于本地消息表**
- **基于支持分布式事务的消息中间件，如RocketMQ等**



可靠消息最终一致性方案，一般适用于异步的服务调用，比如支付成功后，调用积分服务进行积分累加、调用库存服务进行发货等等。总结一下，可靠消息最终一致性方案其实最基本的思想就两点：

- **通过引入消息中间件，保证生产者对消息的100%可靠投递**
- **通过引入Zookeeper，保证消费者能够对未成功消费的消息进行重新消费（消费者要保证自身接口的幂等性）**



可靠消息最终一致性方案是目前业务主流的分布式事务落地方案，其优缺点主要如下：

- **优点**：消息数据独立存储，降低业务系统与消息系统间的耦合

- **缺点**：一次消息发送需要两次请求，业务服务需要提供消息状态查询的回调接口



一般来讲，99%的分布式接口调用不需要做分布式事务，通过监控（邮件、短信告警）、记录日志，就可以事后快速定位问题，然后就是排查、出解决方案、修复数据。因为用分布式事务一定是有成本的，而且这个成本会比较高，特别是对于一些中小型公司。同时，引入分布式事务后，代码复杂度、开发周期会大幅上升，系统性能和吞吐量会大幅下跌，这就导致系统更加更加脆弱，更容易出bug。当然，如果有资源能够持续投入，分布式事务做好了的话，好处就是可以100%保证数据一致性不会出错。



### 本地消息表

![分布式事务-本地消息表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分布式事务-本地消息表.png)

基于本地消息表的分布式事务，是最简便的实现方式，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于eBay。我们来看下面这张图，基于本地消息服务的分布式事务分为三大部分：

- **可靠消息服务**：存储消息，因为通常通过数据库存储，所以也叫本地消息表
- **生产者（上游服务）**：生产者是接口的调用方，生产消息
- **消费者（下游服务）**：消费者是接口的服务方，消费消息
  ![本地消息表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/本地消息表.png)

#### 可靠消息服务

可靠消息服务就是一个单独的服务，有自己的数据库，其主要作用就是存储消息（包含接口调用信息，全局唯一的消息编号），消息通常包含以下状态：

- **待确认**：上游服务发送待确认消息
- **已发送**：上游服务发送确认消息
- **已取消（终态**）：上游服务发送取消消息
- **已完成（终态）**：下游服务确认接口执行完成



#### 生产者

服务调用方（消息生产者）需要调用下游接口时，不直接通过RPC之类的方式调用，而是先生成一条消息，其主要步骤如下：

- 生产者调用接口前，先发送一条待确认消息（一般称为half-msg，包含接口调用信息）给可靠消息服务，可靠消息服务会将这条记录存储到自己的数据库（或本地磁盘），状态为【待确认】
- 生产者执行本地事务，本地事务执行成功并提交后，向可靠消息服务发送一条确认消息；如果本地执行失败，则向消息服务发送一条取消消息
- 可靠消息服务如果收到消息后，修改本地数据库中的那条消息记录的状态改为【已发送】或【已取消】。如果是确认消息，则将消息投递到MQ消息队列；（修改消息状态和投递MQ必须在一个事务里，保证要么都成功要么都失败）

> 为了防止出现：生产者的本地事务执行成功，但是发送确认/取消消息超时的情况。可靠消息服务里一般会提供一个后台定时任务，不停的检查消息表中那些【待确认】的消息，然后回调生产者（上游服务）的一个接口，由生产者确认到底是取消这条消息，还是确认并发送这条消息。

![本地消息表-生产者](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/本地消息表-生产者.png)

通过上面这套机制，可以保证生产者对消息的100%可靠投递。



#### 消费者

服务提供方（消息消费者），从MQ消费消息，然后执行本地事务。执行成功后，反过来通知可靠消息服务，说自己处理成功了，然后可靠消息服务就会把本地消息表中的消息状态置为最终状态【已完成】 。这里要注意两种情况：

- 消费者消费消息失败，或者消费成功但执行本地事务失败。
  针对这种情况，可靠消息服务可以提供一个后台定时任务，不停的检查消息表中那些【已发送】但始终没有变成【已完成】的消息，然后再次投递到MQ，让下游服务来再次处理。也可以引入zookeeper，由消费者通知zookeeper，生产者监听到zookeeper上节点变化后，进行消息的重新投递
- 如果消息重复投递，消息者的接口逻辑需要实现幂等性，保证多次处理一个消息不会插入重复数据或造成业务数据混乱。
  针对这种情况，消费者可以准备一张消息表，用于判重。消费者消费消息后，需要去本地消息表查看这条消息有没处理成功，如果处理成功直接返回成功


![本地消息表-消费者](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/本地消息表-消费者.png)

**总结**

这个方案的优点是简单，但最大的问题在于可靠消息服务是严重依赖于数据库的，即通过数据库的消息表来管理事务，不太适合并发量很高的场景。



### 分布式消息中间件

许多开源的消息中间件都支持分布式事务，比如RocketMQ、Kafka。其思想几乎是和本地消息表/服务实一样的，只不过是将可靠消息服务和MQ功能封装在一起，屏蔽了底层细节，从而更方便用户的使用。这种方案有时也叫做可靠消息最终一致性方案。以RocketMQ为例，消息的发送分成2个阶段：**Prepare阶段**和**确认阶段**。
![分布式消息中间件](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分布式消息中间件.png)

#### prepare阶段

- 生产者发送一个不完整的事务消息——HalfMsg到消息中间件，消息中间件会为这个HalfMsg生成一个全局唯一标识，生产者可以持有标识，以便下一阶段找到这个HalfMsg
- 生产者执行本地事务

> 注意：消费者无法立刻消费HalfMsg，生产者可以对HalfMsg进行Commit或者Rollback来终结事务。只有当Commit了HalfMsg后，消费者才能消费到这条消息。



#### 确认阶段

- 如果生产者执行本地事务成功，就向消息中间件发送一个Commit消息（包含之前HalfMsg的唯一标识），中间件修改HalfMsg的状态为【已提交】，然后通知消费者执行事务
- 如果生产者执行本地事务失败，就向消息中间件发送一个Rollback消息（包含之前HalfMsg的唯一标识），中间件修改HalfMsg的状态为【已取消】

> 消息中间件会定期去向生产者询问，是否可以Commit或者Rollback那些由于错误没有被终结的HalfMsg，以此来结束它们的生命周期，以达成事务最终的一致。之所以需要这个询问机制，是因为生产者可能提交完本地事务，还没来得及对HalfMsg进行Commit或者Rollback，就挂掉了，这样就会处于一种不一致状态。



#### ACK机制

消费者消费完消息后，可能因为自身异常，导致业务执行失败，此时就必须要能够重复消费消息。RocketMQ提供了ACK机制，即RocketMQ只有收到服务消费者的ack message后才认为消费成功。所以，服务消费者可以在自身业务员逻辑执行成功后，向RocketMQ发送ack message，保证消费逻辑执行成功。



### 应用案例

我们最后以一个电子商务支付系统的核心交易链路为示例，来更好的理解下可靠消息最终一致性方案。

**交易链路**

假设我们的系统的核心交易链路如下图。用户支付订单时，首先调用订单服务的对外接口服务，然后开始核心交易链路的调用，依次经过订单业务服务、库存服务、积分服务，全部成功后再通过MQ异步调用仓储服务：
![最终一致性-场景案例-交易链路](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/最终一致性-场景案例-交易链路.png)

上图中，订单业务服务、库存服务、积分服务都是同步调用的，由于是核心链路，我们可以通过上一章中讲解的TCC分布式事务来保证分布式事务的一致性。而调用仓储服务可以异步执行，所以我们依赖RocketMQ来实现分布式事务。



**事务执行**

接着，我们来看下引入RocketMQ来实现分布式事务后，整个系统的业务执行流程发生了哪些变化，整个流程如下图：
![最终一致性-场景案例-事务执行](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/最终一致性-场景案例-事务执行.png)

- 当用户针对订单发起支付时，首先订单接口服务先发送一个half-msg消息给RocketMQ，收到RocketMQ的成功响应（注意，此时仓储服务还不能消费消息，因为half-msg还没有确认）
- 然后，订单接口服务调用核心交易链路，如果其中任一服务执行失败，则先执行内部的TCC事务回滚
- 如果订单接口服务收到链路失败的响应，则向MQ投递一个rollback消息，取消之前的half-msg
- 如果订单接口服务收到链路成功的响应，则向MQ投递一个commit消息，确认之前的half-msg，那仓库服务就可消费消息
- 仓储服务消费消息成功并执行完自身的逻辑后，会向RocketMQ投递一个ack message，以确保消费成功

> 注意，如果因为网络原因，导致RocketMQ始终没有收到订单接口服务对half-msg的commit或rollback消息，RocketMQ就会回调订单接口服务的某个接口，以查询该half-msg究竟是进行commit还是rollback。



## 最大努力通知

最大努力通知的方案实现比较简单，适用于一些最终一致性要求较低的业务。执行流程：

- 系统 A 本地事务执行完之后，发送个消息到 MQ
- 这里会有个专门消费 MQ 的服务，这个服务会消费 MQ 并调用系统 B 的接口
- 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃



## Sagas事务模型

Saga事务模型又叫做长时间运行的事务。其核心思想是**「将长事务拆分为多个本地短事务」**，由Saga事务协调器协调，如果正常结束那就正常完成，如果**「某个步骤失败，则根据相反顺序一次调用补偿操作」**。



## Seate

**XA和Seata区别**

![XA和Seata的区别](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/xa-seata.png)

![XA过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/XA过程.png)

![seata过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/seata过程.png)

Seata 中有三大模块，分别是 TM、RM 和 TC。 其中 TM 和 RM 是作为 Seata 的客户端与业务系统集成在一起，TC 作为 Seata 的服务端独立部署。

![seata流程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/seata流程.png)

在 Seata 中，分布式事务的执行流程：

- TM 开启分布式事务（TM 向 TC 注册全局事务记录）
- 按业务场景，编排数据库、服务等事务内资源（RM 向 TC 汇报资源准备状态 ）
- TM 结束分布式事务，事务一阶段结束（TM 通知 TC 提交/回滚分布式事务）
- TC 汇总事务信息，决定分布式事务是提交还是回滚
- TC 通知所有 RM 提交/回滚 资源，事务二阶段结束

![seata](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/seata.png)

- Transaction Coordinator(TC)

事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。

- Transaction Manager(TM)

控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。

- Resource Manager(RM)

控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。



Seata 会有 4 种分布式事务解决方案，分别是：AT 模式、TCC 模式、Saga 模式和 XA 模式。

![seata发展历程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/seata发展历程.png)



### AT模式

2019年1月，Seata 开源了 AT 模式。AT 模式是一种无侵入的分布式事务解决方案。在 AT 模式下，用户只需关注自己的“业务 SQL”，用户的 “业务 SQL” 作为一阶段，Seata 框架会自动生成事务的二阶段提交和回滚操作。

![AT 模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/AT模式.png)

**AT模式如何做到对业务的无侵入 ：**

- **一阶段**

在一阶段，Seata 会拦截“业务 SQL”，首先解析 SQL 语义，找到“业务 SQL”要更新的业务数据，在业务数据被更新前，将其保存成“before image”，然后执行“业务 SQL”更新业务数据，在业务数据更新之后，再将其保存成“after image”，最后生成行锁。以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性。

![AT模式一阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/AT模式一阶段.png)

- **二阶段提交**

二阶段如果是提交的话，因为“业务 SQL”在一阶段已经提交至数据库， 所以 Seata 框架只需将一阶段保存的快照数据和行锁删掉，完成数据清理即可。

![AT模式二阶段提交](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/AT模式二阶段提交.png)

- **二阶段回滚**

二阶段如果是回滚的话，Seata 就需要回滚一阶段已经执行的“业务 SQL”，还原业务数据。回滚方式便是用“before image”还原业务数据；但在还原前要首先要校验脏写，对比“数据库当前业务数据”和 “after image”，如果两份数据完全一致就说明没有脏写，可以还原业务数据，如果不一致就说明有脏写，出现脏写就需要转人工处理。

![AT模式二阶段回滚](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/AT模式二阶段回滚.png)

AT 模式的一阶段、二阶段提交和回滚均由 Seata 框架自动生成，用户只需编写“业务 SQL”，便能轻松接入分布式事务，AT 模式是一种对业务无任何侵入的分布式事务解决方案。



### TCC模式

2019 年 3 月份，Seata 开源了 TCC 模式，该模式由蚂蚁金服贡献。TCC 模式需要用户根据自己的业务场景实现 Try、Confirm 和 Cancel 三个操作；事务发起方在一阶段 执行 Try 方式，在二阶段提交执行 Confirm 方法，二阶段回滚执行 Cancel 方法。

![TCC模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC模式.png)

TCC 三个方法描述：

- Try：资源的检测和预留
- Confirm：执行的业务操作提交；要求 Try 成功 Confirm 一定要能成功
- Cancel：预留资源释放



**业务模型分 2 阶段设计：**

用户接入 TCC ，最重要的是考虑如何将自己的业务模型拆成两阶段来实现。

以“扣钱”场景为例，在接入 TCC 前，对 A 账户的扣钱，只需一条更新账户余额的 SQL 便能完成；但是在接入 TCC 之后，用户就需要考虑如何将原来一步就能完成的扣钱操作，拆成两阶段，实现成三个方法，并且保证一阶段 Try  成功的话 二阶段 Confirm 一定能成功。

![kqcq](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/kqcq.png)

如上图所示，Try 方法作为一阶段准备方法，需要做资源的检查和预留。在扣钱场景下，Try 要做的事情是就是检查账户余额是否充足，预留转账资金，预留的方式就是冻结 A 账户的 转账资金。Try 方法执行之后，账号 A 余额虽然还是 100，但是其中 30 元已经被冻结了，不能被其他事务使用。

二阶段 Confirm 方法执行真正的扣钱操作。Confirm 会使用 Try 阶段冻结的资金，执行账号扣款。Confirm 方法执行之后，账号 A 在一阶段中冻结的 30 元已经被扣除，账号 A 余额变成 70 元 。

如果二阶段是回滚的话，就需要在 Cancel 方法内释放一阶段 Try 冻结的 30 元，使账号 A 的回到初始状态，100 元全部可用。

用户接入 TCC 模式，最重要的事情就是考虑如何将业务模型拆成 2 阶段，实现成 TCC 的 3 个方法，并且保证 Try 成功 Confirm 一定能成功。相对于 AT 模式，TCC 模式对业务代码有一定的侵入性，但是 TCC 模式无 AT 模式的全局行锁，TCC 性能会比 AT 模式高很多。



### Saga模式

Saga 模式是 Seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。

分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。

![Saga模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Saga模式.png)

Saga 模式下分布式事务通常是由事件驱动的，各个参与者之间是异步执行的，Saga 模式是一种长事务解决方案。



### XA模式

XA 模式是 Seata 将会开源的另一种无侵入的分布式事务解决方案，任何实现了 XA 协议的数据库都可以作为资源参与到分布式事务中，目前主流数据库，例如 MySql、Oracle、DB2、Oceanbase 等均支持 XA 协议。

XA 协议有一系列的指令，分别对应一阶段和二阶段操作。“xa start”和 “xa end”用于开启和结束XA 事务；“xa prepare” 用于预提交 XA 事务，对应一阶段准备；“xa commit”和“xa rollback”用于提交、回滚 XA 事务，对应二阶段提交和回滚。

在 XA 模式下，每一个 XA 事务都是一个事务参与者。分布式事务开启之后，首先在一阶段执行“xa start”、“业务 SQL”、“xa end”和 “xa prepare” 完成 XA 事务的执行和预提交；二阶段如果提交的话就执行 “xa commit”，如果是回滚则执行“xa rollback”。这样便能保证所有 XA 事务都提交或者都回滚。

![XA模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/XA模式.png)

XA 模式下，用户只需关注自己的“业务 SQL”，Seata 框架会自动生成一阶段、二阶段操作；XA 模式的实现如下：

![XA模式实现过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/XA模式实现过程.png)

- **一阶段**

在 XA 模式的一阶段，Seata 会拦截“业务 SQL”，在“业务 SQL”之前开启 XA 事务（“xa start”），然后执行“业务 SQL”，结束 XA 事务“xa end”，最后预提交 XA 事务（“xa prepare”），这样便完成 “业务 SQL”的准备操作。

- **二阶段提交**

执行“xa commit”指令，提交 XA 事务，此时“业务 SQL”才算真正的提交至数据库。

- **二阶段回滚**

执行“xa rollback”指令，回滚 XA 事务，完成“业务 SQL”回滚，释放数据库锁资源。

XA 模式下，用户只需关注“业务 SQL”，Seata 会自动生成一阶段、二阶段提交和二阶段回滚操作。XA 模式和 AT 模式一样是一种对业务无侵入性的解决方案；但与 AT 模式不同的是，XA 模式将快照数据和行锁等通过 XA 指令委托给了数据库来完成，这样 XA 模式实现更加轻量化。



### 性能优化

**优化一：同库模式**

通常一个 TM 会产生一笔主事务日志，一个 RM 会产生一条分支事务日志，每个分布式事务由一个 TM 和若干 RM 组成，一个分布式事务总共会有  1+N 条事务日志（N 为 RM 个数）。

在默认情况下，分布式事务执行过程中客户端将事务日志发送给服务端，服务端再将事务日志存储至数据库中，一条事务日志的存储链路会有 2 次 TCP ，分别是“客户端到服务端”和“服务端到数据库”， 我们称这种模式为异库模式。

在异库模式下，分布式事务存储事务日志总共需要 2*（1+N） 次左右的 TCP 通信。在 RM 数量较少的业务场景下，分布式事务性能还能接受，但有些业务场景下 RM 数量较多，此时事务内 TCP 数量也会增多，分布式事务性能急剧下降。

![异库模式与同库模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/异库模式与同库模式.png)

在事务执行过程中，客户端和服务端进行通信的目的是为了存储事务日志。如果客户端在存储事务日志时，绕过服务端直接将事务日志写入数据库（如上图“同库模式”所示），那么一笔事务日志的存储链路就由原来的 2 次 TCP  变成只需访问一次数据库便可，每条事务日志的存储减少了一次 TCP 通信，整个分布式事务就减少了 N+2 次 TCP  请求，分布式事务的性能大幅提升。**我们将客户端直接将事务日志存储至数据库的模式称为同库模式。**



**优化二：二阶段异步执行**

通常情况下，分布式事务发起方会依次执行一阶段和二阶段方法，然后结束分布式事务，返回结果。如果让分布式事务发起方执行完一阶段之后马上结束并返回结果，二阶段交由独立的线程或者进程异步执行，这样分布式事务的二阶段会晚几秒钟或者若干分钟执行，但事务的最终结果不会有任何改变。

二阶段异步执行之后，分布式事务的最终结果不会有任何影响，但是事务发起方要执行的内容减少一半（一阶段和二阶段都执行变成只执行一阶段），直观的用户感受是分布式事务的性能提升了 50%。

![二阶段异步执行](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/二阶段异步执行.png)



## 应用案例

### 订单库存案例

**电商经典模块**

![电商经典模块](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/电商经典模块.jpg)

**下单流程图**

![下单流程图](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/下单流程图.jpg)

**创建订单和扣件库存场景**

![分布式事务-订单库存](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分布式事务-订单库存.jpg)

**分布式事务-技术方案**

![分布式事务-技术方案](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/分布式事务-技术方案.jpg)



#### MQ消息事务-RocketMQ

先说说MQ的分布式事务，RocketMq在4.3版本已经正式宣布支持分布式事务，在选择Rokcetmq做分布式事务请务必选择4.3以上的版本。

事务消息作为一种异步确保型事务，  将两个事务分支通过 MQ 进行异步解耦，RocketMQ 事务消息的设计流程同样借鉴了两阶段提交理论，整体交互流程如下图所示：

![MQ消息事务-RocketMQ](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/MQ消息事务-RocketMQ.png)

这个时候我们基本可以认为，只有MQ发送方自己的本地事务执行完毕，那么MQ的订阅方必定百分百能够接收到消息，我们再对下单减库存的步骤进行改造。这里涉及到一个异步化的改造，我们理一下如果是同步流程中的各个步骤：

1. 查看商品详情（或购物车）
2. 计算商品价格和目前商品存在库存（生成订单详情）
3. 商品扣库存（调用商品库存服务）
4. 订单确认（生成有效订单）

订单创建完成后，发布一个事件“orderCreate” 到消息队列中，然后由MQ转发给订阅该消息的服务，因为是基于消息事务，我们可以认为订阅该消息的商品模块是百分百能收到这个消息的。

![创建订单-发送MQ消息](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/创建订单-发送MQ消息.jpg)

![创建订单-消费MQ消息](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/创建订单-消费MQ消息.jpg)

商品服务接受到orderCreate消息后就执行扣减库存的操作，注意⚠️，这里可能会有一些不可抗的因素导致扣减库存失败，无论成功或失败，商品服务都将发送一个扣减库存结果的消息“stroeReduce”到消息队列中，订单服务会订阅扣减库存的结果。订单服务收到消息后有两种可能：

- 如果扣减库存成功，将订单状态改为 “确认订单” ，下单成功
- 如果扣减库存失败，将订单状态改为 “失效订单” ，下单失败

![创建订单-回执MQ消息](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/创建订单-回执MQ消息.jpg)

这种模式将确认订单的流程变成异步化，**非常适合在高并发的使用**，但是，切记了，这个需要前端用户体验的一些改变，要配合产品来涉及流程。

上面使用MQ的方式确实是可以完成A和B操作，但是A和B并不是严格一致性，而是最终一致性，我们牺牲掉严格一致性，换来性能的提升，这种很适合在大促高并发场景总使用，但是如果B一直执行不成功，那么一致性也会被破坏，后续应该考虑到更多的兜底方案，方案越细系统就将越复杂。



#### TCC方案

TCC是服务化的二阶段变成模型，每个业务服务都必须实现 try，confirm，calcel三个方法，这三个方式可以对应到SQL事务中Lock，Commit，Rollback。

- try阶段 try只是一个初步的操作，进行初步的确认，它的主要职责是完成所有业务的检查，预留业务资源
- confirm阶段 confirm是在try阶段检查执行完毕后，继续执行的确认操作，必须满足幂等性操作，如果confirm中执行失败，会有事务协调器触发不断的执行，直到满足为止
- cancel是取消执行，在try没通过并释放掉try阶段预留的资源，也必须满足幂等性，跟confirm一样有可能被不断执行

接下来看看，我们的下单扣减库存的流程怎么加入TCC：

![TCC-try-confirm](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC-try-confirm.jpg)

在try的时候，会让库存服务预留n个库存给这个订单使用，让订单服务产生一个“未确认”订单，同时产生这两个预留的资源， 在confirm的时候，会使用在try预留的资源，在TCC事务机制中认为，如果在try阶段能正常预留的资源，那么在confirm一定能完整的提交：

![TCC-try-cancel](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/TCC-try-cancel.jpg)

在try的时候，有任务一方为执行失败，则会执行cancel的接口操作，将在try阶段预留的资源进行释放。



### 下单扣减库存

**传统模式**

![Seata-下单扣减库存-传统模式](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-传统模式.png)

**分库分表**

![Seata-下单扣减库存-分库分表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-分库分表.png)

**Seata 优势**

实现分布式事务的方案比较多，常见的比如基于 `XA` 协议的 `2PC`、`3PC`，基于业务层的 `TCC`，还有应用消息队列 + 消息表实现的最终一致性方案，还有今天要说的 `Seata` 中间件，下边看看各个方案的优缺点。



#### 2PC

基于 XA 协议实现的分布式事务，XA 协议中分为两部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如 Oracle、MYSQL 这些数据库都实现了 XA 接口，而事务管理器则作为一个全局的调度者。

两阶段提交（`2PC`），对业务侵⼊很小，它最⼤的优势就是对使⽤⽅透明，用户可以像使⽤本地事务⼀样使⽤基于 XA 协议的分布式事务，能够严格保障事务 ACID 特性。

![Seata-下单扣减库存-2PC第一阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-2PC第一阶段.png)

可 `2PC`的缺点也是显而易见，它是一个强一致性的同步阻塞协议，事务执⾏过程中需要将所需资源全部锁定，也就是俗称的 `刚性事务`。所以它比较适⽤于执⾏时间确定的短事务，整体性能比较差。

一旦事务协调者宕机或者发生网络抖动，会让参与者一直处于锁定资源的状态或者只有一部分参与者提交成功，导致数据的不一致。因此，在⾼并发性能⾄上的场景中，基于 XA 协议的分布式事务并不是最佳选择。

![Seata-下单扣减库存-2PC第二阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-2PC第二阶段.png)



#### 3PC

三段提交（`3PC`）是二阶段提交（`2PC`）的一种改进版本 ，为解决两阶段提交协议的阻塞问题，上边提到两段提交，当协调者崩溃时，参与者不能做出最后的选择，就会一直保持阻塞锁定资源。

`2PC` 中只有协调者有超时机制，`3PC` 在协调者和参与者中都引入了超时机制，协调者出现故障后，参与者就不会一直阻塞。而且在第一阶段和第二阶段中又插入了一个准备阶段（如下图，看着有点啰嗦），保证了在最后提交阶段之前各参与节点的状态是一致的。

![Seata-下单扣减库存-3PC](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-3PC.png)

虽然 `3PC` 用超时机制，解决了协调者故障后参与者的阻塞问题，但与此同时却多了一次网络通信，性能上反而变得更差，也不太推荐。



#### TCC

所谓的 `TCC` 编程模式，也是两阶段提交的一个变种，不同的是 `TCC` 为在业务层编写代码实现的两阶段提交。`TCC` 分别指 `Try`、`Confirm`、`Cancel` ，一个业务操作要对应的写这三个方法。

以下单扣库存为例，`Try` 阶段去占库存，`Confirm` 阶段则实际扣库存，如果库存扣减失败 `Cancel` 阶段进行回滚，释放库存。

TCC 不存在资源阻塞的问题，因为每个方法都直接进行事务的提交，一旦出现异常通过则 `Cancel` 来进行回滚补偿，这也就是常说的补偿性事务。

原本一个方法，现在却需要三个方法来支持，可以看到 TCC 对业务的侵入性很强，而且这种模式并不能很好地被复用，会导致开发量激增。还要考虑到网络波动等原因，为保证请求一定送达都会有重试机制，所以考虑到接口的幂等性。



#### 消息事务

消息事务（最终一致性）其实就是基于消息中间件的两阶段提交，将本地事务和发消息放在同一个事务里，保证本地操作和发送消息同时成功。下单扣库存原理图：

![Seata-下单扣减库存-消息事务](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-消息事务.png)

- 订单系统向 `MQ` 发送一条预备扣减库存消息， `MQ` 保存预备消息并返回成功 `ACK`
- 接收到预备消息执行成功 `ACK`，订单系统执行本地下单操作，为防止消息发送成功而本地事务失败，订单系统会实现 `MQ` 的回调接口，其内不断的检查本地事务是否执行成功，如果失败则 `rollback` 回滚预备消息；成功则对消息进行最终 `commit` 提交。
- 库存系统消费扣减库存消息，执行本地事务，如果扣减失败，消息会重新投，一旦超出重试次数，则本地表持久化失败消息，并启动定时任务做补偿。

基于消息中间件的两阶段提交方案，通常用在高并发场景下使用，牺牲数据的强一致性换取性能的大幅提升，不过实现这种方式的成本和复杂度是比较高的，还要看实际业务情况。



#### Seata

`Seata` 也是从两段提交演变而来的一种分布式事务解决方案，提供了 `AT`、`TCC`、`SAGA` 和 `XA` 等事务模式，这里重点介绍 `AT`模式。既然 `Seata` 是两段提交，那我们看看它在每个阶段都做了点啥？下边我们还以下单扣库存、扣余额举例。

![Seata-下单扣减库存-Seata](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-下单扣减库存-Seata.png)

先介绍 `Seata` 分布式事务的几种角色：

- `Transaction Coordinator(TC)`:  全局事务协调者，用来协调全局事务和各个分支事务（不同服务）的状态， 驱动全局事务和各个分支事务的回滚或提交
- `Transaction Manager™`:  事务管理者，业务层中用来开启/提交/回滚一个整体事务（在调用服务的方法中用注解开启事务）
- `Resource Manager(RM)`:  资源管理者，一般指业务数据库代表了一个分支事务（`Branch Transaction`），管理分支事务与 `TC` 进行协调注册分支事务并且汇报分支事务的状态，驱动分支事务的提交或回滚

Seata 实现分布式事务，设计了一个关键角色 `UNDO_LOG` （回滚日志记录表），我们在每个应用分布式事务的业务库中创建这张表，这个表的核心作用就是，将业务数据在更新前后的数据镜像组织成回滚日志，备份在 `UNDO_LOG` 表中，以便业务异常能随时回滚。



**第一个阶段**

比如：下边我们更新 `user` 表的 `name` 字段。

```mysql
update user set name = '小富最帅' where name = '程序员内点事'
```

首先 Seata 的 `JDBC` 数据源代理通过对业务 SQL 解析，提取 SQL 的元数据，也就是得到 SQL 的类型（`UPDATE`），表（`user`），条件（`where name = '程序员内点事'`）等相关的信息。

![Seata-第一阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-第一阶段.png)

先查询数据前镜像，根据解析得到的条件信息，生成查询语句，定位一条数据。

```mysql
select  name from user where name = '程序员内点事'
```

![Seata-第一阶段-数据前镜像](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-第一阶段-数据前镜像.png)

紧接着执行业务 SQL，根据前镜像数据主键查询出后镜像数据

```mysql
select name from user where id = 1
```

![Seata-第一阶段-数据后镜像](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-第一阶段-数据后镜像.png)

把业务数据在更新前后的数据镜像组织成回滚日志，将业务数据的更新和回滚日志在同一个本地事务中提交，分别插入到业务表和 `UNDO_LOG` 表中。

回滚记录数据格式如下：包括 `afterImage` 前镜像、`beforeImage` 后镜像、 `branchId` 分支事务ID、`xid` 全局事务ID

```json
{
    "branchId":641789253,
    "xid":"xid:xxx",
    "undoItems":[
        {
            "afterImage":{
                "rows":[
                    {
                        "fields":[
                            {
                                "name":"id",
                                "type":4,
                                "value":1
                            }
                        ]
                    }
                ],
                "tableName":"product"
            },
            "beforeImage":{
                "rows":[
                    {
                        "fields":[
                            {
                                "name":"id",
                                "type":4,
                                "value":1
                            }
                        ]
                    }
                ],
                "tableName":"product"
            },
            "sqlType":"UPDATE"
        }
    ]
}
```

这样就可以保证，任何提交的业务数据的更新一定有相应的回滚日志。

在本地事务提交前，各分支事务需向 `全局事务协调者` TC 注册分支 ( `Branch Id`) ，为要修改的记录申请 **全局锁** ，要为这条数据加锁，利用 `SELECT FOR UPDATE` 语句。而如果一直拿不到锁那就需要回滚本地事务。TM 开启事务后会生成全局唯一的 `XID`，会在各个调用的服务间进行传递。

有了这样的机制，本地事务分支（`Branch Transaction`）便可以在全局事务的第一阶段提交，并马上释放本地事务锁定的资源。相比于传统的 `XA` 事务在第二阶段释放资源，`Seata` 降低了锁范围提高效率，即使第二阶段发生异常需要回滚，也可以快速 从`UNDO_LOG` 表中找到对应回滚数据并反解析成 SQL 来达到回滚补偿。

最后本地事务提交，业务数据的更新和前面生成的 UNDO LOG 数据一并提交，并将本地事务提交的结果上报给全局事务协调者 TC。



**第二个阶段**

第二阶段是根据各分支的决议做提交或回滚：

如果决议是全局提交，此时各分支事务已提交并成功，这时 `全局事务协调者（TC）` 会向分支发送第二阶段的请求。收到 TC 的分支提交请求，该请求会被放入一个异步任务队列中，并马上返回提交成功结果给 TC。异步队列中会异步和批量地根据 `Branch ID` 查找并删除相应 `UNDO LOG` 回滚记录。

![Seata-第二阶段](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-第二阶段.png)

如果决议是全局回滚，过程比全局提交麻烦一点，`RM` 服务方收到 `TC` 全局协调者发来的回滚请求，通过 `XID` 和 `Branch ID` 找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚。

注意：这里删除回滚日志记录操作，一定是在本地业务事务执行之后

![Seata-第二阶段-分支回滚](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/Seata-第二阶段-分支回滚.png)

上边说了几种分布式事务各自的优缺点，下边实践一下分布式事务中间 Seata 感受一下。



### Seata 实践

Seata 是一个需独立部署的中间件，所以先搭 Seata Server，这里以最新的 `seata-server-1.4.0` 版本为例，下载地址：`https://seata.io/en-us/blog/download.html`。解压后的文件我们只需要关心 `\seata\conf` 目录下的 `file.conf` 和  `registry.conf` 文件。



#### Seata Server

**file.conf**

`file.conf` 文件用于配置持久化事务日志的模式，目前提供 `file`、`db`、`redis` 三种方式。

![SeataServer-file.conf](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-file.conf.png)

**注意**：在选择 `db` 方式后，需要在对应数据库创建 `globalTable`（持久化全局事务）、`branchTable`（持久化各提交分支的事务）、 `lockTable`（持久化各分支锁定资源事务）三张表。

```mysql
-- the table to store GlobalSession data
-- 持久化全局事务
CREATE TABLE IF NOT EXISTS `global_table`
(
    `xid`                       VARCHAR(128) NOT NULL,
    `transaction_id`            BIGINT,
    `status`                    TINYINT      NOT NULL,
    `application_id`            VARCHAR(32),
    `transaction_service_group` VARCHAR(32),
    `transaction_name`          VARCHAR(128),
    `timeout`                   INT,
    `begin_time`                BIGINT,
    `application_data`          VARCHAR(2000),
    `gmt_create`                DATETIME,
    `gmt_modified`              DATETIME,
    PRIMARY KEY (`xid`),
    KEY `idx_gmt_modified_status` (`gmt_modified`, `status`),
    KEY `idx_transaction_id` (`transaction_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

-- the table to store BranchSession data
-- 持久化各提交分支的事务
CREATE TABLE IF NOT EXISTS `branch_table`
(
    `branch_id`         BIGINT       NOT NULL,
    `xid`               VARCHAR(128) NOT NULL,
    `transaction_id`    BIGINT,
    `resource_group_id` VARCHAR(32),
    `resource_id`       VARCHAR(256),
    `branch_type`       VARCHAR(8),
    `status`            TINYINT,
    `client_id`         VARCHAR(64),
    `application_data`  VARCHAR(2000),
    `gmt_create`        DATETIME(6),
    `gmt_modified`      DATETIME(6),
    PRIMARY KEY (`branch_id`),
    KEY `idx_xid` (`xid`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;

-- the table to store lock data
-- 持久化每个分支锁表事务
CREATE TABLE IF NOT EXISTS `lock_table`
(
    `row_key`        VARCHAR(128) NOT NULL,
    `xid`            VARCHAR(96),
    `transaction_id` BIGINT,
    `branch_id`      BIGINT       NOT NULL,
    `resource_id`    VARCHAR(256),
    `table_name`     VARCHAR(32),
    `pk`             VARCHAR(36),
    `gmt_create`     DATETIME,
    `gmt_modified`   DATETIME,
    PRIMARY KEY (`row_key`),
    KEY `idx_branch_id` (`branch_id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;
```



**registry.conf**

`registry.conf` 文件设置 注册中心 和 配置中心：

目前注册中心支持 `nacos` 、`eureka`、`redis`、`zk`、`consul`、`etcd3`、`sofa` 七种，这里我使用的 `eureka`作为注册中心 ；配置中心支持 `nacos` 、`apollo`、`zk`、`consul`、`etcd3` 五种方式。

![SeataServer-registry.conf](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-registry.conf.png)

配置完以后在 `\seata\bin` 目录下启动 `seata-server` 即可，到这 `Seata` 的服务端就搭建好了。



#### Seata Client

`Seata Server` 环境搭建完，接下来我们新建三个服务 `order-server`（下单服务）、`storage-server`（扣减库存服务）、`account-server`（账户金额服务），分别服务注册到 `eureka`。

每个服务的大体核心配置如下：

```yaml
spring:
    application:
        name: storage-server
    cloud:
        alibaba:
            seata:
                tx-service-group: my_test_tx_group
    datasource:
        driver-class-name: com.mysql.jdbc.Driver
        url: jdbc:mysql://47.93.6.1:3306/seat-storage
        username: root
        password: root

# eureka 注册中心
eureka:
    client:
        serviceUrl:
            defaultZone: http://${eureka.instance.hostname}:8761/eureka/
    instance:
        hostname: 47.93.6.5
        prefer-ip-address: true
```

业务大致流程：用户发起下单请求，本地 order 订单服务创建订单记录，并通过 `RPC` 远程调用 `storage` 扣减库存服务和 `account` 扣账户余额服务，只有三个服务同时执行成功，才是一个完整的下单流程。如果某个服执行失败，则其他服务全部回滚。Seata 对业务代码的侵入性非常小，代码中使用只需用 `@GlobalTransactional` 注解开启一个全局事务即可。

```java
@Override
@GlobalTransactional(name = "create-order", rollbackFor = Exception.class)
public void create(Order order) {

    String xid = RootContext.getXID();

    LOGGER.info("------->交易开始");
    //本地方法
    orderDao.create(order);

    //远程方法 扣减库存
    storageApi.decrease(order.getProductId(), order.getCount());

    //远程方法 扣减账户余额
    LOGGER.info("------->扣减账户开始order中");
    accountApi.decrease(order.getUserId(), order.getMoney());
    LOGGER.info("------->扣减账户结束order中");

    LOGGER.info("------->交易结束");
    LOGGER.info("全局事务 xid： {}", xid);
}
```

前边说过 Seata AT 模式实现分布式事务，必须在相关的业务库中创建 `undo_log` 表来存数据回滚日志，表结构如下：

```mysql
-- for AT mode you must to init this sql for you business database. the seata server not need it.
CREATE TABLE IF NOT EXISTS `undo_log`
(
    `id`            BIGINT(20)   NOT NULL AUTO_INCREMENT COMMENT 'increment id',
    `branch_id`     BIGINT(20)   NOT NULL COMMENT 'branch transaction id',
    `xid`           VARCHAR(100) NOT NULL COMMENT 'global transaction id',
    `context`       VARCHAR(128) NOT NULL COMMENT 'undo_log context,such as serialization',
    `rollback_info` LONGBLOB     NOT NULL COMMENT 'rollback info',
    `log_status`    INT(11)      NOT NULL COMMENT '0:normal status,1:defense status',
    `log_created`   DATETIME     NOT NULL COMMENT 'create datetime',
    `log_modified`  DATETIME     NOT NULL COMMENT 'modify datetime',
    PRIMARY KEY (`id`),
    UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)
) ENGINE = InnoDB
  AUTO_INCREMENT = 1
  DEFAULT CHARSET = utf8 COMMENT ='AT transaction mode undo table';
```



#### 测试Seata

项目中的服务调用过程如下图：

![SeataServer-服务调用过程](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-服务调用过程.png)

启动各个服务后，我们直接请求下单接口看看效果，只要 `order` 订单表创建记录成功，`storage` 库存表 `used` 字段数量递增、`account` 余额表 `used` 字段数量递增则表示下单流程成功。

![SeataServer-测试表-原始数据](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-测试表-原始数据.png)

请求后正向流程是没问题的，数据和预想的一样

![SeataServer-测试表-下单数据](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-测试表-下单数据.png)而且发现 `TM` 事务管理者 `order-server` 服务的控制台也打印出了两阶段提交的日志

![SeataServer-控制台两次提交](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-控制台两次提交.png)那么再看看如果其中一个服务异常，会正常回滚呢？在 `account-server` 服务中模拟超时异常，看能否实现全局事务回滚。

![SeataServer-全局事务回滚](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-全局事务回滚.png)

发现数据全没执行成功，说明全局事务回滚也成功了

![SeataServer-回滚后数据表](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-回滚后数据表.png)

那看一下 `undo_log` 回滚记录表的变化情况，由于 `Seata` 删除回滚日志的速度很快，所以要想在表中看见回滚日志，必须要在某一个服务上打断点才看的更明显。

![SeataServer-回滚记录](C:/Users/DELL/Downloads/lemon-guide-main/images/Solution/SeataServer-回滚记录.png)



**总结**

上边简单介绍了 `2PC`、`3PC`、`TCC`、`MQ`、`Seata` 这五种分布式事务解决方案，还详细的实践了 `Seata` 中间件。但不管我们选哪一种方案，在项目中应用都要谨慎再谨慎，除特定的数据强一致性场景外，能不用尽量就不要用，因为无论它们性能如何优越，一旦项目套上分布式事务，整体效率会几倍的下降，在高并发情况下弊端尤为明显。


